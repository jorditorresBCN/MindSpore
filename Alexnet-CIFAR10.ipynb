{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet-CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zvxf cifar-10-binary.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import context\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target='CPU', enable_mem_reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'num_classes': 10,\n",
    "    'learning_rate': 0.002,\n",
    "    'momentum': 0.9,\n",
    "    'epoch_size': 1, # remember that you are using a Docker with only 1 CPU \n",
    "    'batch_size': 32,\n",
    "    'buffer_size': 1000,\n",
    "    'image_height': 227,\n",
    "    'image_width': 227,\n",
    "    'save_checkpoint_steps': 1562,\n",
    "    'keep_checkpoint_max': 10,\n",
    "})\n",
    "\n",
    "data_path=\"cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset.transforms.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, status=\"train\"):\n",
    "    \"\"\"\n",
    "    create dataset for train or test\n",
    "    \"\"\"\n",
    "    cifar_ds = ds.Cifar10Dataset(data_path)\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "\n",
    "    resize_op = CV.Resize((cfg.image_height, cfg.image_width))\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    normalize_op = CV.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    if status == \"train\":\n",
    "        random_crop_op = CV.RandomCrop([32, 32], [4, 4, 4, 4])\n",
    "        random_horizontal_op = CV.RandomHorizontalFlip()\n",
    "    channel_swap_op = CV.HWC2CHW()\n",
    "    typecast_op = C.TypeCast(mstype.int32)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"label\", operations=typecast_op)\n",
    "    if status == \"train\":\n",
    "        cifar_ds = cifar_ds.map(input_columns=\"image\", operations=random_crop_op)\n",
    "        cifar_ds = cifar_ds.map(input_columns=\"image\", operations=random_horizontal_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=resize_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=rescale_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=normalize_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=channel_swap_op)\n",
    "\n",
    "    cifar_ds = cifar_ds.shuffle(buffer_size=cfg.buffer_size)\n",
    "    cifar_ds = cifar_ds.batch(batch_size, drop_remainder=True)\n",
    "    cifar_ds = cifar_ds.repeat(repeat_size)\n",
    "    return cifar_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mindspore.ops.operations as P\n",
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0, pad_mode=\"valid\"):\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=pad_mode)\n",
    "\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)\n",
    "\n",
    "def weight_variable():\n",
    "    return TruncatedNormal(0.02)  # 0.02\n",
    "\n",
    "\n",
    "class AlexNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    Alexnet\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.conv1 = conv(3, 96, 11, stride=4)\n",
    "        self.conv2 = conv(96, 256, 5, pad_mode=\"same\")\n",
    "        self.conv3 = conv(256, 384, 3, pad_mode=\"same\")\n",
    "        self.conv4 = conv(384, 384, 3, pad_mode=\"same\")\n",
    "        self.conv5 = conv(384, 256, 3, pad_mode=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = fc_with_initialize(6*6*256, 4096)\n",
    "        self.fc2 = fc_with_initialize(4096, 4096)\n",
    "        self.fc3 = fc_with_initialize(4096, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.2953937\n",
      "epoch: 1 step: 2, loss is 2.3085976\n",
      "epoch: 1 step: 3, loss is 2.2658746\n",
      "epoch: 1 step: 4, loss is 2.300441\n",
      "epoch: 1 step: 5, loss is 2.3148353\n",
      "epoch: 1 step: 6, loss is 2.316042\n",
      "epoch: 1 step: 7, loss is 2.297753\n",
      "epoch: 1 step: 8, loss is 2.303807\n",
      "epoch: 1 step: 9, loss is 2.3032458\n",
      "epoch: 1 step: 10, loss is 2.2855701\n",
      "epoch: 1 step: 11, loss is 2.23874\n",
      "epoch: 1 step: 12, loss is 2.253618\n",
      "epoch: 1 step: 13, loss is 2.333591\n",
      "epoch: 1 step: 14, loss is 2.3551157\n",
      "epoch: 1 step: 15, loss is 2.2862926\n",
      "epoch: 1 step: 16, loss is 2.251695\n",
      "epoch: 1 step: 17, loss is 2.254908\n",
      "epoch: 1 step: 18, loss is 2.2822368\n",
      "epoch: 1 step: 19, loss is 2.1970465\n",
      "epoch: 1 step: 20, loss is 2.3318756\n",
      "epoch: 1 step: 21, loss is 2.330506\n",
      "epoch: 1 step: 22, loss is 2.264245\n",
      "epoch: 1 step: 23, loss is 2.324888\n",
      "epoch: 1 step: 24, loss is 2.2817268\n",
      "epoch: 1 step: 25, loss is 2.2624626\n",
      "epoch: 1 step: 26, loss is 2.2509553\n",
      "epoch: 1 step: 27, loss is 2.2216113\n",
      "epoch: 1 step: 28, loss is 2.2016218\n",
      "epoch: 1 step: 29, loss is 2.1825454\n",
      "epoch: 1 step: 30, loss is 2.2056568\n",
      "epoch: 1 step: 31, loss is 2.2247083\n",
      "epoch: 1 step: 32, loss is 2.1924288\n",
      "epoch: 1 step: 33, loss is 2.2836053\n",
      "epoch: 1 step: 34, loss is 2.286761\n",
      "epoch: 1 step: 35, loss is 2.1618114\n",
      "epoch: 1 step: 36, loss is 2.254922\n",
      "epoch: 1 step: 37, loss is 2.1946304\n",
      "epoch: 1 step: 38, loss is 2.2184184\n",
      "epoch: 1 step: 39, loss is 2.1842964\n",
      "epoch: 1 step: 40, loss is 2.251532\n",
      "epoch: 1 step: 41, loss is 2.0939374\n",
      "epoch: 1 step: 42, loss is 2.14599\n",
      "epoch: 1 step: 43, loss is 2.1154423\n",
      "epoch: 1 step: 44, loss is 2.1873918\n",
      "epoch: 1 step: 45, loss is 2.0749235\n",
      "epoch: 1 step: 46, loss is 2.0619633\n",
      "epoch: 1 step: 47, loss is 2.2493734\n",
      "epoch: 1 step: 48, loss is 2.1219852\n",
      "epoch: 1 step: 49, loss is 2.169244\n",
      "epoch: 1 step: 50, loss is 2.1081157\n",
      "epoch: 1 step: 51, loss is 2.1549232\n",
      "epoch: 1 step: 52, loss is 2.0120773\n",
      "epoch: 1 step: 53, loss is 1.8923278\n",
      "epoch: 1 step: 54, loss is 2.104608\n",
      "epoch: 1 step: 55, loss is 2.158559\n",
      "epoch: 1 step: 56, loss is 2.0065403\n",
      "epoch: 1 step: 57, loss is 2.0473733\n",
      "epoch: 1 step: 58, loss is 2.2007453\n",
      "epoch: 1 step: 59, loss is 2.134887\n",
      "epoch: 1 step: 60, loss is 1.9868646\n",
      "epoch: 1 step: 61, loss is 2.259479\n",
      "epoch: 1 step: 62, loss is 2.3620834\n",
      "epoch: 1 step: 63, loss is 2.1898448\n",
      "epoch: 1 step: 64, loss is 1.989954\n",
      "epoch: 1 step: 65, loss is 2.2467458\n",
      "epoch: 1 step: 66, loss is 2.162488\n",
      "epoch: 1 step: 67, loss is 2.2501254\n",
      "epoch: 1 step: 68, loss is 2.0934951\n",
      "epoch: 1 step: 69, loss is 2.0864117\n",
      "epoch: 1 step: 70, loss is 2.2122505\n",
      "epoch: 1 step: 71, loss is 2.1061378\n",
      "epoch: 1 step: 72, loss is 1.9307425\n",
      "epoch: 1 step: 73, loss is 2.0900178\n",
      "epoch: 1 step: 74, loss is 2.1382482\n",
      "epoch: 1 step: 75, loss is 2.0780993\n",
      "epoch: 1 step: 76, loss is 1.9901756\n",
      "epoch: 1 step: 77, loss is 2.0145822\n",
      "epoch: 1 step: 78, loss is 2.1704373\n",
      "epoch: 1 step: 79, loss is 1.9741923\n",
      "epoch: 1 step: 80, loss is 1.8547578\n",
      "epoch: 1 step: 81, loss is 2.1910138\n",
      "epoch: 1 step: 82, loss is 2.0284765\n",
      "epoch: 1 step: 83, loss is 2.0655224\n",
      "epoch: 1 step: 84, loss is 2.1478145\n",
      "epoch: 1 step: 85, loss is 1.9791049\n",
      "epoch: 1 step: 86, loss is 2.005084\n",
      "epoch: 1 step: 87, loss is 1.9398267\n",
      "epoch: 1 step: 88, loss is 2.070086\n",
      "epoch: 1 step: 89, loss is 2.0570023\n",
      "epoch: 1 step: 90, loss is 2.0294318\n",
      "epoch: 1 step: 91, loss is 1.9992056\n",
      "epoch: 1 step: 92, loss is 2.0332763\n",
      "epoch: 1 step: 93, loss is 1.886129\n",
      "epoch: 1 step: 94, loss is 2.218742\n",
      "epoch: 1 step: 95, loss is 1.9540417\n",
      "epoch: 1 step: 96, loss is 1.8459411\n",
      "epoch: 1 step: 97, loss is 2.011393\n",
      "epoch: 1 step: 98, loss is 2.18225\n",
      "epoch: 1 step: 99, loss is 2.1121488\n",
      "epoch: 1 step: 100, loss is 1.9523768\n",
      "epoch: 1 step: 101, loss is 2.1163523\n",
      "epoch: 1 step: 102, loss is 1.9617069\n",
      "epoch: 1 step: 103, loss is 1.9190843\n",
      "epoch: 1 step: 104, loss is 1.7791393\n",
      "epoch: 1 step: 105, loss is 2.212161\n",
      "epoch: 1 step: 106, loss is 1.9045184\n",
      "epoch: 1 step: 107, loss is 1.94627\n",
      "epoch: 1 step: 108, loss is 2.1972027\n",
      "epoch: 1 step: 109, loss is 2.0290494\n",
      "epoch: 1 step: 110, loss is 1.7719891\n",
      "epoch: 1 step: 111, loss is 1.8103582\n",
      "epoch: 1 step: 112, loss is 1.8474799\n",
      "epoch: 1 step: 113, loss is 2.002304\n",
      "epoch: 1 step: 114, loss is 1.8833907\n",
      "epoch: 1 step: 115, loss is 1.9822475\n",
      "epoch: 1 step: 116, loss is 1.8452325\n",
      "epoch: 1 step: 117, loss is 1.850957\n",
      "epoch: 1 step: 118, loss is 1.9900337\n",
      "epoch: 1 step: 119, loss is 1.9823365\n",
      "epoch: 1 step: 120, loss is 1.8576059\n",
      "epoch: 1 step: 121, loss is 2.0648763\n",
      "epoch: 1 step: 122, loss is 1.7544261\n",
      "epoch: 1 step: 123, loss is 1.888627\n",
      "epoch: 1 step: 124, loss is 1.9882581\n",
      "epoch: 1 step: 125, loss is 2.0441124\n",
      "epoch: 1 step: 126, loss is 2.0749817\n",
      "epoch: 1 step: 127, loss is 1.8200982\n",
      "epoch: 1 step: 128, loss is 1.8486698\n",
      "epoch: 1 step: 129, loss is 2.1836288\n",
      "epoch: 1 step: 130, loss is 1.9914489\n",
      "epoch: 1 step: 131, loss is 1.952226\n",
      "epoch: 1 step: 132, loss is 1.8489057\n",
      "epoch: 1 step: 133, loss is 1.8998032\n",
      "epoch: 1 step: 134, loss is 2.075188\n",
      "epoch: 1 step: 135, loss is 2.110746\n",
      "epoch: 1 step: 136, loss is 1.6423187\n",
      "epoch: 1 step: 137, loss is 1.9648622\n",
      "epoch: 1 step: 138, loss is 2.2045264\n",
      "epoch: 1 step: 139, loss is 2.1020372\n",
      "epoch: 1 step: 140, loss is 1.728505\n",
      "epoch: 1 step: 141, loss is 1.9080224\n",
      "epoch: 1 step: 142, loss is 2.020515\n",
      "epoch: 1 step: 143, loss is 1.9441197\n",
      "epoch: 1 step: 144, loss is 1.9807212\n",
      "epoch: 1 step: 145, loss is 2.0572958\n",
      "epoch: 1 step: 146, loss is 2.0416207\n",
      "epoch: 1 step: 147, loss is 1.9098773\n",
      "epoch: 1 step: 148, loss is 1.973473\n",
      "epoch: 1 step: 149, loss is 1.8435458\n",
      "epoch: 1 step: 150, loss is 1.8276995\n",
      "epoch: 1 step: 151, loss is 2.1343896\n",
      "epoch: 1 step: 152, loss is 1.7318481\n",
      "epoch: 1 step: 153, loss is 1.7286652\n",
      "epoch: 1 step: 154, loss is 1.981864\n",
      "epoch: 1 step: 155, loss is 1.9597224\n",
      "epoch: 1 step: 156, loss is 1.9012728\n",
      "epoch: 1 step: 157, loss is 2.2994673\n",
      "epoch: 1 step: 158, loss is 1.9515721\n",
      "epoch: 1 step: 159, loss is 2.037381\n",
      "epoch: 1 step: 160, loss is 1.7945291\n",
      "epoch: 1 step: 161, loss is 1.8323274\n",
      "epoch: 1 step: 162, loss is 1.7062402\n",
      "epoch: 1 step: 163, loss is 1.9811027\n",
      "epoch: 1 step: 164, loss is 2.0372348\n",
      "epoch: 1 step: 165, loss is 2.3072674\n",
      "epoch: 1 step: 166, loss is 1.9156106\n",
      "epoch: 1 step: 167, loss is 1.8445805\n",
      "epoch: 1 step: 168, loss is 1.9061192\n",
      "epoch: 1 step: 169, loss is 1.8589977\n",
      "epoch: 1 step: 170, loss is 2.06203\n",
      "epoch: 1 step: 171, loss is 2.0359497\n",
      "epoch: 1 step: 172, loss is 1.9438077\n",
      "epoch: 1 step: 173, loss is 2.141649\n",
      "epoch: 1 step: 174, loss is 1.8780732\n",
      "epoch: 1 step: 175, loss is 1.7403171\n",
      "epoch: 1 step: 176, loss is 1.8319298\n",
      "epoch: 1 step: 177, loss is 1.9955035\n",
      "epoch: 1 step: 178, loss is 1.8108748\n",
      "epoch: 1 step: 179, loss is 1.8549439\n",
      "epoch: 1 step: 180, loss is 1.8752891\n",
      "epoch: 1 step: 181, loss is 1.7840159\n",
      "epoch: 1 step: 182, loss is 1.5318146\n",
      "epoch: 1 step: 183, loss is 1.9655395\n",
      "epoch: 1 step: 184, loss is 1.788062\n",
      "epoch: 1 step: 185, loss is 1.8274674\n",
      "epoch: 1 step: 186, loss is 1.8202053\n",
      "epoch: 1 step: 187, loss is 2.032733\n",
      "epoch: 1 step: 188, loss is 2.0073633\n",
      "epoch: 1 step: 189, loss is 1.7777411\n",
      "epoch: 1 step: 190, loss is 2.326075\n",
      "epoch: 1 step: 191, loss is 1.8438483\n",
      "epoch: 1 step: 192, loss is 1.8323245\n",
      "epoch: 1 step: 193, loss is 2.0775194\n",
      "epoch: 1 step: 194, loss is 1.9952731\n",
      "epoch: 1 step: 195, loss is 2.0781243\n",
      "epoch: 1 step: 196, loss is 1.7220422\n",
      "epoch: 1 step: 197, loss is 2.1321735\n",
      "epoch: 1 step: 198, loss is 1.9828417\n",
      "epoch: 1 step: 199, loss is 1.8309592\n",
      "epoch: 1 step: 200, loss is 1.827671\n",
      "epoch: 1 step: 201, loss is 1.802125\n",
      "epoch: 1 step: 202, loss is 1.9128264\n",
      "epoch: 1 step: 203, loss is 1.8629725\n",
      "epoch: 1 step: 204, loss is 2.0977418\n",
      "epoch: 1 step: 205, loss is 1.8652114\n",
      "epoch: 1 step: 206, loss is 2.0703406\n",
      "epoch: 1 step: 207, loss is 1.8522393\n",
      "epoch: 1 step: 208, loss is 1.8374603\n",
      "epoch: 1 step: 209, loss is 1.9243053\n",
      "epoch: 1 step: 210, loss is 1.9097991\n",
      "epoch: 1 step: 211, loss is 1.8451083\n",
      "epoch: 1 step: 212, loss is 1.9910259\n",
      "epoch: 1 step: 213, loss is 1.9900208\n",
      "epoch: 1 step: 214, loss is 1.8651772\n",
      "epoch: 1 step: 215, loss is 1.7651929\n",
      "epoch: 1 step: 216, loss is 1.9373423\n",
      "epoch: 1 step: 217, loss is 1.8849502\n",
      "epoch: 1 step: 218, loss is 1.8499042\n",
      "epoch: 1 step: 219, loss is 2.0732543\n",
      "epoch: 1 step: 220, loss is 1.8399553\n",
      "epoch: 1 step: 221, loss is 1.9087183\n",
      "epoch: 1 step: 222, loss is 1.8223689\n",
      "epoch: 1 step: 223, loss is 1.8141627\n",
      "epoch: 1 step: 224, loss is 1.8603615\n",
      "epoch: 1 step: 225, loss is 1.8161266\n",
      "epoch: 1 step: 226, loss is 1.7428157\n",
      "epoch: 1 step: 227, loss is 1.8740054\n",
      "epoch: 1 step: 228, loss is 1.613842\n",
      "epoch: 1 step: 229, loss is 1.861928\n",
      "epoch: 1 step: 230, loss is 1.8928077\n",
      "epoch: 1 step: 231, loss is 1.6633087\n",
      "epoch: 1 step: 232, loss is 1.6568085\n",
      "epoch: 1 step: 233, loss is 1.8143446\n",
      "epoch: 1 step: 234, loss is 1.8639245\n",
      "epoch: 1 step: 235, loss is 2.2065277\n",
      "epoch: 1 step: 236, loss is 2.051233\n",
      "epoch: 1 step: 237, loss is 1.8524425\n",
      "epoch: 1 step: 238, loss is 1.9969585\n",
      "epoch: 1 step: 239, loss is 1.90142\n",
      "epoch: 1 step: 240, loss is 1.7771612\n",
      "epoch: 1 step: 241, loss is 1.9964483\n",
      "epoch: 1 step: 242, loss is 1.8991538\n",
      "epoch: 1 step: 243, loss is 1.7402167\n",
      "epoch: 1 step: 244, loss is 1.525041\n",
      "epoch: 1 step: 245, loss is 1.669254\n",
      "epoch: 1 step: 246, loss is 1.7906984\n",
      "epoch: 1 step: 247, loss is 1.9914607\n",
      "epoch: 1 step: 248, loss is 1.8423434\n",
      "epoch: 1 step: 249, loss is 1.9787208\n",
      "epoch: 1 step: 250, loss is 1.7622068\n",
      "epoch: 1 step: 251, loss is 1.8512241\n",
      "epoch: 1 step: 252, loss is 2.0866172\n",
      "epoch: 1 step: 253, loss is 1.8048347\n",
      "epoch: 1 step: 254, loss is 1.7653\n",
      "epoch: 1 step: 255, loss is 1.9355221\n",
      "epoch: 1 step: 256, loss is 1.8579589\n",
      "epoch: 1 step: 257, loss is 1.9230182\n",
      "epoch: 1 step: 258, loss is 1.8840033\n",
      "epoch: 1 step: 259, loss is 1.6567906\n",
      "epoch: 1 step: 260, loss is 1.7702274\n",
      "epoch: 1 step: 261, loss is 1.8296212\n",
      "epoch: 1 step: 262, loss is 1.7660221\n",
      "epoch: 1 step: 263, loss is 1.8803124\n",
      "epoch: 1 step: 264, loss is 1.6633999\n",
      "epoch: 1 step: 265, loss is 1.5539023\n",
      "epoch: 1 step: 266, loss is 2.0440407\n",
      "epoch: 1 step: 267, loss is 1.9746217\n",
      "epoch: 1 step: 268, loss is 1.7041634\n",
      "epoch: 1 step: 269, loss is 1.851102\n",
      "epoch: 1 step: 270, loss is 1.8177664\n",
      "epoch: 1 step: 271, loss is 1.8841653\n",
      "epoch: 1 step: 272, loss is 2.109151\n",
      "epoch: 1 step: 273, loss is 1.6751896\n",
      "epoch: 1 step: 274, loss is 2.1341667\n",
      "epoch: 1 step: 275, loss is 1.8444\n",
      "epoch: 1 step: 276, loss is 1.9840115\n",
      "epoch: 1 step: 277, loss is 2.0324135\n",
      "epoch: 1 step: 278, loss is 1.6537575\n",
      "epoch: 1 step: 279, loss is 1.9452327\n",
      "epoch: 1 step: 280, loss is 1.6429405\n",
      "epoch: 1 step: 281, loss is 1.9142429\n",
      "epoch: 1 step: 282, loss is 2.148651\n",
      "epoch: 1 step: 283, loss is 1.8552085\n",
      "epoch: 1 step: 284, loss is 1.7307503\n",
      "epoch: 1 step: 285, loss is 1.7414498\n",
      "epoch: 1 step: 286, loss is 1.93045\n",
      "epoch: 1 step: 287, loss is 1.5486523\n",
      "epoch: 1 step: 288, loss is 1.672767\n",
      "epoch: 1 step: 289, loss is 1.7589334\n",
      "epoch: 1 step: 290, loss is 1.8563111\n",
      "epoch: 1 step: 291, loss is 1.7922043\n",
      "epoch: 1 step: 292, loss is 2.0919905\n",
      "epoch: 1 step: 293, loss is 1.7624054\n",
      "epoch: 1 step: 294, loss is 2.001844\n",
      "epoch: 1 step: 295, loss is 1.8818033\n",
      "epoch: 1 step: 296, loss is 1.8056786\n",
      "epoch: 1 step: 297, loss is 1.8285466\n",
      "epoch: 1 step: 298, loss is 1.8496478\n",
      "epoch: 1 step: 299, loss is 1.8691623\n",
      "epoch: 1 step: 300, loss is 1.7340813\n",
      "epoch: 1 step: 301, loss is 1.9564908\n",
      "epoch: 1 step: 302, loss is 1.765803\n",
      "epoch: 1 step: 303, loss is 1.9777329\n",
      "epoch: 1 step: 304, loss is 1.8024522\n",
      "epoch: 1 step: 305, loss is 1.9051671\n",
      "epoch: 1 step: 306, loss is 1.8638082\n",
      "epoch: 1 step: 307, loss is 1.6666863\n",
      "epoch: 1 step: 308, loss is 2.0160775\n",
      "epoch: 1 step: 309, loss is 1.7959152\n",
      "epoch: 1 step: 310, loss is 1.7880218\n",
      "epoch: 1 step: 311, loss is 1.9259663\n",
      "epoch: 1 step: 312, loss is 1.6861312\n",
      "epoch: 1 step: 313, loss is 1.5809466\n",
      "epoch: 1 step: 314, loss is 1.6752069\n",
      "epoch: 1 step: 315, loss is 1.8762783\n",
      "epoch: 1 step: 316, loss is 2.060692\n",
      "epoch: 1 step: 317, loss is 1.9347987\n",
      "epoch: 1 step: 318, loss is 1.5242766\n",
      "epoch: 1 step: 319, loss is 1.4975146\n",
      "epoch: 1 step: 320, loss is 1.8631145\n",
      "epoch: 1 step: 321, loss is 1.5719054\n",
      "epoch: 1 step: 322, loss is 1.8257923\n",
      "epoch: 1 step: 323, loss is 1.7722142\n",
      "epoch: 1 step: 324, loss is 1.9164281\n",
      "epoch: 1 step: 325, loss is 1.8999599\n",
      "epoch: 1 step: 326, loss is 1.720266\n",
      "epoch: 1 step: 327, loss is 1.8859621\n",
      "epoch: 1 step: 328, loss is 1.8571359\n",
      "epoch: 1 step: 329, loss is 1.6736883\n",
      "epoch: 1 step: 330, loss is 1.7075034\n",
      "epoch: 1 step: 331, loss is 1.7832577\n",
      "epoch: 1 step: 332, loss is 1.6427114\n",
      "epoch: 1 step: 333, loss is 1.5668579\n",
      "epoch: 1 step: 334, loss is 1.7961541\n",
      "epoch: 1 step: 335, loss is 1.724879\n",
      "epoch: 1 step: 336, loss is 1.5477047\n",
      "epoch: 1 step: 337, loss is 1.9016218\n",
      "epoch: 1 step: 338, loss is 1.5749981\n",
      "epoch: 1 step: 339, loss is 1.4125082\n",
      "epoch: 1 step: 340, loss is 1.6502968\n",
      "epoch: 1 step: 341, loss is 1.7123429\n",
      "epoch: 1 step: 342, loss is 1.8338081\n",
      "epoch: 1 step: 343, loss is 1.6644807\n",
      "epoch: 1 step: 344, loss is 1.4563935\n",
      "epoch: 1 step: 345, loss is 1.600465\n",
      "epoch: 1 step: 346, loss is 1.5166448\n",
      "epoch: 1 step: 347, loss is 1.617812\n",
      "epoch: 1 step: 348, loss is 1.634129\n",
      "epoch: 1 step: 349, loss is 2.116783\n",
      "epoch: 1 step: 350, loss is 1.9197844\n",
      "epoch: 1 step: 351, loss is 1.7541987\n",
      "epoch: 1 step: 352, loss is 1.8690903\n",
      "epoch: 1 step: 353, loss is 1.9996046\n",
      "epoch: 1 step: 354, loss is 1.8480062\n",
      "epoch: 1 step: 355, loss is 1.5510572\n",
      "epoch: 1 step: 356, loss is 2.1855066\n",
      "epoch: 1 step: 357, loss is 1.6413325\n",
      "epoch: 1 step: 358, loss is 1.7577857\n",
      "epoch: 1 step: 359, loss is 1.7367928\n",
      "epoch: 1 step: 360, loss is 1.758703\n",
      "epoch: 1 step: 361, loss is 1.7387911\n",
      "epoch: 1 step: 362, loss is 1.7346052\n",
      "epoch: 1 step: 363, loss is 1.5594596\n",
      "epoch: 1 step: 364, loss is 1.7744318\n",
      "epoch: 1 step: 365, loss is 1.5704776\n",
      "epoch: 1 step: 366, loss is 1.6485745\n",
      "epoch: 1 step: 367, loss is 1.7325519\n",
      "epoch: 1 step: 368, loss is 1.7384737\n",
      "epoch: 1 step: 369, loss is 1.6199194\n",
      "epoch: 1 step: 370, loss is 1.6272385\n",
      "epoch: 1 step: 371, loss is 1.61644\n",
      "epoch: 1 step: 372, loss is 1.5253744\n",
      "epoch: 1 step: 373, loss is 1.4946531\n",
      "epoch: 1 step: 374, loss is 1.8385378\n",
      "epoch: 1 step: 375, loss is 1.6345559\n",
      "epoch: 1 step: 376, loss is 1.5848109\n",
      "epoch: 1 step: 377, loss is 1.4960454\n",
      "epoch: 1 step: 378, loss is 2.0588367\n",
      "epoch: 1 step: 379, loss is 1.9509405\n",
      "epoch: 1 step: 380, loss is 1.7912238\n",
      "epoch: 1 step: 381, loss is 1.8053898\n",
      "epoch: 1 step: 382, loss is 1.9287953\n",
      "epoch: 1 step: 383, loss is 1.4737406\n",
      "epoch: 1 step: 384, loss is 1.7973872\n",
      "epoch: 1 step: 385, loss is 1.8043603\n",
      "epoch: 1 step: 386, loss is 1.7783818\n",
      "epoch: 1 step: 387, loss is 1.6285965\n",
      "epoch: 1 step: 388, loss is 1.7232287\n",
      "epoch: 1 step: 389, loss is 1.6588916\n",
      "epoch: 1 step: 390, loss is 1.6504228\n",
      "epoch: 1 step: 391, loss is 1.6107863\n",
      "epoch: 1 step: 392, loss is 1.581547\n",
      "epoch: 1 step: 393, loss is 1.514551\n",
      "epoch: 1 step: 394, loss is 2.0607588\n",
      "epoch: 1 step: 395, loss is 1.8192031\n",
      "epoch: 1 step: 396, loss is 1.7112099\n",
      "epoch: 1 step: 397, loss is 1.9194537\n",
      "epoch: 1 step: 398, loss is 2.0354571\n",
      "epoch: 1 step: 399, loss is 1.5740958\n",
      "epoch: 1 step: 400, loss is 1.5355206\n",
      "epoch: 1 step: 401, loss is 1.4083247\n",
      "epoch: 1 step: 402, loss is 1.9107435\n",
      "epoch: 1 step: 403, loss is 1.6686283\n",
      "epoch: 1 step: 404, loss is 1.8428712\n",
      "epoch: 1 step: 405, loss is 1.6046627\n",
      "epoch: 1 step: 406, loss is 2.1367102\n",
      "epoch: 1 step: 407, loss is 1.6635447\n",
      "epoch: 1 step: 408, loss is 1.7370712\n",
      "epoch: 1 step: 409, loss is 1.6419871\n",
      "epoch: 1 step: 410, loss is 1.6379406\n",
      "epoch: 1 step: 411, loss is 1.8566093\n",
      "epoch: 1 step: 412, loss is 1.4973533\n",
      "epoch: 1 step: 413, loss is 1.8113326\n",
      "epoch: 1 step: 414, loss is 1.5724756\n",
      "epoch: 1 step: 415, loss is 1.554715\n",
      "epoch: 1 step: 416, loss is 1.7431319\n",
      "epoch: 1 step: 417, loss is 1.9345958\n",
      "epoch: 1 step: 418, loss is 1.8079048\n",
      "epoch: 1 step: 419, loss is 1.71705\n",
      "epoch: 1 step: 420, loss is 1.5785699\n",
      "epoch: 1 step: 421, loss is 1.6903168\n",
      "epoch: 1 step: 422, loss is 1.4698974\n",
      "epoch: 1 step: 423, loss is 1.9678719\n",
      "epoch: 1 step: 424, loss is 1.6415414\n",
      "epoch: 1 step: 425, loss is 1.7766786\n",
      "epoch: 1 step: 426, loss is 1.4364249\n",
      "epoch: 1 step: 427, loss is 1.716529\n",
      "epoch: 1 step: 428, loss is 1.7346272\n",
      "epoch: 1 step: 429, loss is 1.3923767\n",
      "epoch: 1 step: 430, loss is 1.8094814\n",
      "epoch: 1 step: 431, loss is 1.6417761\n",
      "epoch: 1 step: 432, loss is 1.6224458\n",
      "epoch: 1 step: 433, loss is 1.5095676\n",
      "epoch: 1 step: 434, loss is 1.5198029\n",
      "epoch: 1 step: 435, loss is 1.5660816\n",
      "epoch: 1 step: 436, loss is 1.4917575\n",
      "epoch: 1 step: 437, loss is 2.0266926\n",
      "epoch: 1 step: 438, loss is 1.39771\n",
      "epoch: 1 step: 439, loss is 1.462084\n",
      "epoch: 1 step: 440, loss is 1.8567274\n",
      "epoch: 1 step: 441, loss is 1.8628798\n",
      "epoch: 1 step: 442, loss is 1.5650839\n",
      "epoch: 1 step: 443, loss is 1.9859749\n",
      "epoch: 1 step: 444, loss is 1.5339304\n",
      "epoch: 1 step: 445, loss is 1.8436807\n",
      "epoch: 1 step: 446, loss is 1.7562258\n",
      "epoch: 1 step: 447, loss is 1.5568725\n",
      "epoch: 1 step: 448, loss is 1.7066208\n",
      "epoch: 1 step: 449, loss is 1.5732117\n",
      "epoch: 1 step: 450, loss is 1.7177479\n",
      "epoch: 1 step: 451, loss is 1.6772493\n",
      "epoch: 1 step: 452, loss is 1.942762\n",
      "epoch: 1 step: 453, loss is 1.8231263\n",
      "epoch: 1 step: 454, loss is 1.6496384\n",
      "epoch: 1 step: 455, loss is 1.7438644\n",
      "epoch: 1 step: 456, loss is 1.7620206\n",
      "epoch: 1 step: 457, loss is 1.5242195\n",
      "epoch: 1 step: 458, loss is 1.8908972\n",
      "epoch: 1 step: 459, loss is 1.548168\n",
      "epoch: 1 step: 460, loss is 1.6561725\n",
      "epoch: 1 step: 461, loss is 1.910254\n",
      "epoch: 1 step: 462, loss is 1.6325837\n",
      "epoch: 1 step: 463, loss is 1.6968678\n",
      "epoch: 1 step: 464, loss is 1.5216231\n",
      "epoch: 1 step: 465, loss is 1.7021109\n",
      "epoch: 1 step: 466, loss is 1.726642\n",
      "epoch: 1 step: 467, loss is 1.6591992\n",
      "epoch: 1 step: 468, loss is 1.6777321\n",
      "epoch: 1 step: 469, loss is 1.4979159\n",
      "epoch: 1 step: 470, loss is 2.0593421\n",
      "epoch: 1 step: 471, loss is 1.8966967\n",
      "epoch: 1 step: 472, loss is 1.6693029\n",
      "epoch: 1 step: 473, loss is 1.7117426\n",
      "epoch: 1 step: 474, loss is 1.3686832\n",
      "epoch: 1 step: 475, loss is 1.5867816\n",
      "epoch: 1 step: 476, loss is 1.8746108\n",
      "epoch: 1 step: 477, loss is 1.6120113\n",
      "epoch: 1 step: 478, loss is 1.8567291\n",
      "epoch: 1 step: 479, loss is 1.5490121\n",
      "epoch: 1 step: 480, loss is 1.7017525\n",
      "epoch: 1 step: 481, loss is 2.0768268\n",
      "epoch: 1 step: 482, loss is 2.0605862\n",
      "epoch: 1 step: 483, loss is 1.6394491\n",
      "epoch: 1 step: 484, loss is 1.6050214\n",
      "epoch: 1 step: 485, loss is 1.7398679\n",
      "epoch: 1 step: 486, loss is 1.673312\n",
      "epoch: 1 step: 487, loss is 1.8927501\n",
      "epoch: 1 step: 488, loss is 2.0730386\n",
      "epoch: 1 step: 489, loss is 1.9059219\n",
      "epoch: 1 step: 490, loss is 1.5345085\n",
      "epoch: 1 step: 491, loss is 1.7581958\n",
      "epoch: 1 step: 492, loss is 1.8010566\n",
      "epoch: 1 step: 493, loss is 1.5301471\n",
      "epoch: 1 step: 494, loss is 1.7720464\n",
      "epoch: 1 step: 495, loss is 1.5580181\n",
      "epoch: 1 step: 496, loss is 1.49004\n",
      "epoch: 1 step: 497, loss is 1.6568999\n",
      "epoch: 1 step: 498, loss is 1.5503186\n",
      "epoch: 1 step: 499, loss is 1.4278953\n",
      "epoch: 1 step: 500, loss is 1.7080457\n",
      "epoch: 1 step: 501, loss is 1.4423128\n",
      "epoch: 1 step: 502, loss is 1.5018885\n",
      "epoch: 1 step: 503, loss is 1.793016\n",
      "epoch: 1 step: 504, loss is 1.8681797\n",
      "epoch: 1 step: 505, loss is 1.9578134\n",
      "epoch: 1 step: 506, loss is 1.2944082\n",
      "epoch: 1 step: 507, loss is 1.5197959\n",
      "epoch: 1 step: 508, loss is 1.6913612\n",
      "epoch: 1 step: 509, loss is 1.4922719\n",
      "epoch: 1 step: 510, loss is 1.7314193\n",
      "epoch: 1 step: 511, loss is 1.532507\n",
      "epoch: 1 step: 512, loss is 1.5572336\n",
      "epoch: 1 step: 513, loss is 1.7848595\n",
      "epoch: 1 step: 514, loss is 1.4829344\n",
      "epoch: 1 step: 515, loss is 1.413065\n",
      "epoch: 1 step: 516, loss is 1.4668139\n",
      "epoch: 1 step: 517, loss is 1.5041685\n",
      "epoch: 1 step: 518, loss is 1.6182618\n",
      "epoch: 1 step: 519, loss is 1.6357757\n",
      "epoch: 1 step: 520, loss is 1.6010303\n",
      "epoch: 1 step: 521, loss is 1.4138317\n",
      "epoch: 1 step: 522, loss is 1.6066461\n",
      "epoch: 1 step: 523, loss is 1.4762162\n",
      "epoch: 1 step: 524, loss is 1.9062731\n",
      "epoch: 1 step: 525, loss is 1.7574117\n",
      "epoch: 1 step: 526, loss is 1.5505817\n",
      "epoch: 1 step: 527, loss is 1.4202051\n",
      "epoch: 1 step: 528, loss is 1.7491589\n",
      "epoch: 1 step: 529, loss is 1.4865493\n",
      "epoch: 1 step: 530, loss is 2.1999204\n",
      "epoch: 1 step: 531, loss is 1.7456019\n",
      "epoch: 1 step: 532, loss is 1.6406932\n",
      "epoch: 1 step: 533, loss is 1.7156361\n",
      "epoch: 1 step: 534, loss is 1.4589442\n",
      "epoch: 1 step: 535, loss is 2.026978\n",
      "epoch: 1 step: 536, loss is 1.8451791\n",
      "epoch: 1 step: 537, loss is 1.6669468\n",
      "epoch: 1 step: 538, loss is 1.5147315\n",
      "epoch: 1 step: 539, loss is 1.6440196\n",
      "epoch: 1 step: 540, loss is 1.7873204\n",
      "epoch: 1 step: 541, loss is 1.5332581\n",
      "epoch: 1 step: 542, loss is 1.6124322\n",
      "epoch: 1 step: 543, loss is 1.8063849\n",
      "epoch: 1 step: 544, loss is 1.5409698\n",
      "epoch: 1 step: 545, loss is 1.6197833\n",
      "epoch: 1 step: 546, loss is 1.7599207\n",
      "epoch: 1 step: 547, loss is 1.4213688\n",
      "epoch: 1 step: 548, loss is 1.316022\n",
      "epoch: 1 step: 549, loss is 1.6836654\n",
      "epoch: 1 step: 550, loss is 1.4213417\n",
      "epoch: 1 step: 551, loss is 1.8003869\n",
      "epoch: 1 step: 552, loss is 1.5240144\n",
      "epoch: 1 step: 553, loss is 1.5493741\n",
      "epoch: 1 step: 554, loss is 1.7925822\n",
      "epoch: 1 step: 555, loss is 1.396449\n",
      "epoch: 1 step: 556, loss is 1.4692104\n",
      "epoch: 1 step: 557, loss is 1.5226561\n",
      "epoch: 1 step: 558, loss is 1.728805\n",
      "epoch: 1 step: 559, loss is 1.5398196\n",
      "epoch: 1 step: 560, loss is 1.6006724\n",
      "epoch: 1 step: 561, loss is 1.6129742\n",
      "epoch: 1 step: 562, loss is 1.6601784\n",
      "epoch: 1 step: 563, loss is 1.9174933\n",
      "epoch: 1 step: 564, loss is 1.5498588\n",
      "epoch: 1 step: 565, loss is 2.0100465\n",
      "epoch: 1 step: 566, loss is 1.851261\n",
      "epoch: 1 step: 567, loss is 1.795276\n",
      "epoch: 1 step: 568, loss is 1.6057094\n",
      "epoch: 1 step: 569, loss is 1.7288336\n",
      "epoch: 1 step: 570, loss is 1.624942\n",
      "epoch: 1 step: 571, loss is 1.8558139\n",
      "epoch: 1 step: 572, loss is 1.6278551\n",
      "epoch: 1 step: 573, loss is 2.0700922\n",
      "epoch: 1 step: 574, loss is 1.7204654\n",
      "epoch: 1 step: 575, loss is 1.8094703\n",
      "epoch: 1 step: 576, loss is 1.5305281\n",
      "epoch: 1 step: 577, loss is 1.7988877\n",
      "epoch: 1 step: 578, loss is 1.724912\n",
      "epoch: 1 step: 579, loss is 1.5209455\n",
      "epoch: 1 step: 580, loss is 1.3713784\n",
      "epoch: 1 step: 581, loss is 1.5206134\n",
      "epoch: 1 step: 582, loss is 1.3545821\n",
      "epoch: 1 step: 583, loss is 1.7507408\n",
      "epoch: 1 step: 584, loss is 1.4438217\n",
      "epoch: 1 step: 585, loss is 2.1346526\n",
      "epoch: 1 step: 586, loss is 1.7216994\n",
      "epoch: 1 step: 587, loss is 1.5063398\n",
      "epoch: 1 step: 588, loss is 1.4330395\n",
      "epoch: 1 step: 589, loss is 1.70398\n",
      "epoch: 1 step: 590, loss is 1.5569859\n",
      "epoch: 1 step: 591, loss is 1.3887876\n",
      "epoch: 1 step: 592, loss is 1.6553464\n",
      "epoch: 1 step: 593, loss is 1.5086919\n",
      "epoch: 1 step: 594, loss is 1.3209934\n",
      "epoch: 1 step: 595, loss is 1.5858351\n",
      "epoch: 1 step: 596, loss is 1.5231041\n",
      "epoch: 1 step: 597, loss is 1.7486879\n",
      "epoch: 1 step: 598, loss is 1.8527286\n",
      "epoch: 1 step: 599, loss is 1.5841875\n",
      "epoch: 1 step: 600, loss is 1.5093772\n",
      "epoch: 1 step: 601, loss is 1.448736\n",
      "epoch: 1 step: 602, loss is 1.5644354\n",
      "epoch: 1 step: 603, loss is 1.4785591\n",
      "epoch: 1 step: 604, loss is 1.7347162\n",
      "epoch: 1 step: 605, loss is 1.783885\n",
      "epoch: 1 step: 606, loss is 1.6791239\n",
      "epoch: 1 step: 607, loss is 1.346007\n",
      "epoch: 1 step: 608, loss is 1.7345926\n",
      "epoch: 1 step: 609, loss is 1.6310501\n",
      "epoch: 1 step: 610, loss is 1.8404016\n",
      "epoch: 1 step: 611, loss is 1.5644746\n",
      "epoch: 1 step: 612, loss is 1.6778919\n",
      "epoch: 1 step: 613, loss is 1.5770147\n",
      "epoch: 1 step: 614, loss is 1.5399649\n",
      "epoch: 1 step: 615, loss is 1.7853702\n",
      "epoch: 1 step: 616, loss is 1.6800345\n",
      "epoch: 1 step: 617, loss is 1.5118511\n",
      "epoch: 1 step: 618, loss is 1.7595667\n",
      "epoch: 1 step: 619, loss is 2.0047643\n",
      "epoch: 1 step: 620, loss is 1.5399067\n",
      "epoch: 1 step: 621, loss is 1.6557544\n",
      "epoch: 1 step: 622, loss is 1.6728595\n",
      "epoch: 1 step: 623, loss is 1.6106217\n",
      "epoch: 1 step: 624, loss is 1.5257177\n",
      "epoch: 1 step: 625, loss is 1.4428669\n",
      "epoch: 1 step: 626, loss is 1.5632182\n",
      "epoch: 1 step: 627, loss is 1.6117876\n",
      "epoch: 1 step: 628, loss is 1.750833\n",
      "epoch: 1 step: 629, loss is 1.7042972\n",
      "epoch: 1 step: 630, loss is 1.4291517\n",
      "epoch: 1 step: 631, loss is 1.9634691\n",
      "epoch: 1 step: 632, loss is 1.2873236\n",
      "epoch: 1 step: 633, loss is 1.4301594\n",
      "epoch: 1 step: 634, loss is 1.6362392\n",
      "epoch: 1 step: 635, loss is 1.6307598\n",
      "epoch: 1 step: 636, loss is 1.707738\n",
      "epoch: 1 step: 637, loss is 1.6319358\n",
      "epoch: 1 step: 638, loss is 1.5394981\n",
      "epoch: 1 step: 639, loss is 1.658644\n",
      "epoch: 1 step: 640, loss is 1.6787324\n",
      "epoch: 1 step: 641, loss is 1.6934378\n",
      "epoch: 1 step: 642, loss is 1.8068124\n",
      "epoch: 1 step: 643, loss is 1.767863\n",
      "epoch: 1 step: 644, loss is 1.4168144\n",
      "epoch: 1 step: 645, loss is 1.7051785\n",
      "epoch: 1 step: 646, loss is 1.4535172\n",
      "epoch: 1 step: 647, loss is 1.5919495\n",
      "epoch: 1 step: 648, loss is 1.430015\n",
      "epoch: 1 step: 649, loss is 1.5509098\n",
      "epoch: 1 step: 650, loss is 1.6434463\n",
      "epoch: 1 step: 651, loss is 1.5534693\n",
      "epoch: 1 step: 652, loss is 1.7721754\n",
      "epoch: 1 step: 653, loss is 1.6457767\n",
      "epoch: 1 step: 654, loss is 1.8762695\n",
      "epoch: 1 step: 655, loss is 1.7752082\n",
      "epoch: 1 step: 656, loss is 1.4725478\n",
      "epoch: 1 step: 657, loss is 1.5973706\n",
      "epoch: 1 step: 658, loss is 1.5212113\n",
      "epoch: 1 step: 659, loss is 1.4847629\n",
      "epoch: 1 step: 660, loss is 1.4575784\n",
      "epoch: 1 step: 661, loss is 1.6452706\n",
      "epoch: 1 step: 662, loss is 1.9545403\n",
      "epoch: 1 step: 663, loss is 1.4834509\n",
      "epoch: 1 step: 664, loss is 1.355108\n",
      "epoch: 1 step: 665, loss is 2.0009441\n",
      "epoch: 1 step: 666, loss is 1.5313823\n",
      "epoch: 1 step: 667, loss is 1.6984473\n",
      "epoch: 1 step: 668, loss is 1.7953751\n",
      "epoch: 1 step: 669, loss is 1.6425488\n",
      "epoch: 1 step: 670, loss is 1.5586867\n",
      "epoch: 1 step: 671, loss is 1.4020199\n",
      "epoch: 1 step: 672, loss is 1.5567929\n",
      "epoch: 1 step: 673, loss is 1.8440031\n",
      "epoch: 1 step: 674, loss is 1.5595386\n",
      "epoch: 1 step: 675, loss is 1.8364742\n",
      "epoch: 1 step: 676, loss is 1.5031661\n",
      "epoch: 1 step: 677, loss is 1.5257386\n",
      "epoch: 1 step: 678, loss is 1.739639\n",
      "epoch: 1 step: 679, loss is 1.6435533\n",
      "epoch: 1 step: 680, loss is 1.6548984\n",
      "epoch: 1 step: 681, loss is 1.618201\n",
      "epoch: 1 step: 682, loss is 1.656991\n",
      "epoch: 1 step: 683, loss is 1.3957232\n",
      "epoch: 1 step: 684, loss is 1.2632223\n",
      "epoch: 1 step: 685, loss is 1.6590543\n",
      "epoch: 1 step: 686, loss is 1.4806596\n",
      "epoch: 1 step: 687, loss is 1.5132396\n",
      "epoch: 1 step: 688, loss is 1.6886134\n",
      "epoch: 1 step: 689, loss is 1.3975778\n",
      "epoch: 1 step: 690, loss is 1.7337601\n",
      "epoch: 1 step: 691, loss is 1.3029016\n",
      "epoch: 1 step: 692, loss is 1.3514881\n",
      "epoch: 1 step: 693, loss is 1.6821121\n",
      "epoch: 1 step: 694, loss is 1.4648452\n",
      "epoch: 1 step: 695, loss is 1.7568985\n",
      "epoch: 1 step: 696, loss is 1.661032\n",
      "epoch: 1 step: 697, loss is 1.8142686\n",
      "epoch: 1 step: 698, loss is 1.529785\n",
      "epoch: 1 step: 699, loss is 1.6913344\n",
      "epoch: 1 step: 700, loss is 1.6851771\n",
      "epoch: 1 step: 701, loss is 1.5193843\n",
      "epoch: 1 step: 702, loss is 1.4696083\n",
      "epoch: 1 step: 703, loss is 1.5332122\n",
      "epoch: 1 step: 704, loss is 1.6226119\n",
      "epoch: 1 step: 705, loss is 1.6513206\n",
      "epoch: 1 step: 706, loss is 1.9822719\n",
      "epoch: 1 step: 707, loss is 1.370932\n",
      "epoch: 1 step: 708, loss is 1.8442209\n",
      "epoch: 1 step: 709, loss is 1.7887908\n",
      "epoch: 1 step: 710, loss is 1.4065113\n",
      "epoch: 1 step: 711, loss is 1.6818314\n",
      "epoch: 1 step: 712, loss is 1.3363222\n",
      "epoch: 1 step: 713, loss is 1.5865539\n",
      "epoch: 1 step: 714, loss is 1.4788735\n",
      "epoch: 1 step: 715, loss is 1.4101617\n",
      "epoch: 1 step: 716, loss is 1.6780242\n",
      "epoch: 1 step: 717, loss is 1.6399851\n",
      "epoch: 1 step: 718, loss is 1.5943309\n",
      "epoch: 1 step: 719, loss is 1.1992241\n",
      "epoch: 1 step: 720, loss is 1.9683734\n",
      "epoch: 1 step: 721, loss is 1.492056\n",
      "epoch: 1 step: 722, loss is 1.5627688\n",
      "epoch: 1 step: 723, loss is 1.4215935\n",
      "epoch: 1 step: 724, loss is 1.5153044\n",
      "epoch: 1 step: 725, loss is 1.6498101\n",
      "epoch: 1 step: 726, loss is 1.3135368\n",
      "epoch: 1 step: 727, loss is 1.7058548\n",
      "epoch: 1 step: 728, loss is 1.5363283\n",
      "epoch: 1 step: 729, loss is 1.6936321\n",
      "epoch: 1 step: 730, loss is 1.7033616\n",
      "epoch: 1 step: 731, loss is 1.3599997\n",
      "epoch: 1 step: 732, loss is 1.8693696\n",
      "epoch: 1 step: 733, loss is 1.4681793\n",
      "epoch: 1 step: 734, loss is 1.7056527\n",
      "epoch: 1 step: 735, loss is 1.4193733\n",
      "epoch: 1 step: 736, loss is 1.3574374\n",
      "epoch: 1 step: 737, loss is 1.6476698\n",
      "epoch: 1 step: 738, loss is 1.8395407\n",
      "epoch: 1 step: 739, loss is 1.5970901\n",
      "epoch: 1 step: 740, loss is 1.439661\n",
      "epoch: 1 step: 741, loss is 1.6136123\n",
      "epoch: 1 step: 742, loss is 1.5702056\n",
      "epoch: 1 step: 743, loss is 1.4824821\n",
      "epoch: 1 step: 744, loss is 1.5155652\n",
      "epoch: 1 step: 745, loss is 1.598095\n",
      "epoch: 1 step: 746, loss is 1.5058448\n",
      "epoch: 1 step: 747, loss is 1.5208472\n",
      "epoch: 1 step: 748, loss is 1.6932117\n",
      "epoch: 1 step: 749, loss is 1.6200215\n",
      "epoch: 1 step: 750, loss is 1.518956\n",
      "epoch: 1 step: 751, loss is 1.3468256\n",
      "epoch: 1 step: 752, loss is 1.6286198\n",
      "epoch: 1 step: 753, loss is 1.5259975\n",
      "epoch: 1 step: 754, loss is 1.7620018\n",
      "epoch: 1 step: 755, loss is 1.239891\n",
      "epoch: 1 step: 756, loss is 1.9573507\n",
      "epoch: 1 step: 757, loss is 1.6315168\n",
      "epoch: 1 step: 758, loss is 1.6322517\n",
      "epoch: 1 step: 759, loss is 1.49044\n",
      "epoch: 1 step: 760, loss is 1.6527176\n",
      "epoch: 1 step: 761, loss is 1.4711865\n",
      "epoch: 1 step: 762, loss is 1.3921305\n",
      "epoch: 1 step: 763, loss is 1.5802062\n",
      "epoch: 1 step: 764, loss is 1.7642957\n",
      "epoch: 1 step: 765, loss is 1.564839\n",
      "epoch: 1 step: 766, loss is 1.507184\n",
      "epoch: 1 step: 767, loss is 1.363442\n",
      "epoch: 1 step: 768, loss is 1.696336\n",
      "epoch: 1 step: 769, loss is 1.2774658\n",
      "epoch: 1 step: 770, loss is 1.4993981\n",
      "epoch: 1 step: 771, loss is 1.3436745\n",
      "epoch: 1 step: 772, loss is 1.5277327\n",
      "epoch: 1 step: 773, loss is 1.4360858\n",
      "epoch: 1 step: 774, loss is 1.4666569\n",
      "epoch: 1 step: 775, loss is 1.9192762\n",
      "epoch: 1 step: 776, loss is 1.9905349\n",
      "epoch: 1 step: 777, loss is 1.5712372\n",
      "epoch: 1 step: 778, loss is 1.6197469\n",
      "epoch: 1 step: 779, loss is 1.9817\n",
      "epoch: 1 step: 780, loss is 1.5764227\n",
      "epoch: 1 step: 781, loss is 1.6360743\n",
      "epoch: 1 step: 782, loss is 1.6288254\n",
      "epoch: 1 step: 783, loss is 1.7688912\n",
      "epoch: 1 step: 784, loss is 1.5974962\n",
      "epoch: 1 step: 785, loss is 1.3607066\n",
      "epoch: 1 step: 786, loss is 1.3488579\n",
      "epoch: 1 step: 787, loss is 1.5698868\n",
      "epoch: 1 step: 788, loss is 1.2891138\n",
      "epoch: 1 step: 789, loss is 1.5856487\n",
      "epoch: 1 step: 790, loss is 1.687817\n",
      "epoch: 1 step: 791, loss is 1.5984776\n",
      "epoch: 1 step: 792, loss is 1.8751414\n",
      "epoch: 1 step: 793, loss is 1.2226471\n",
      "epoch: 1 step: 794, loss is 1.413846\n",
      "epoch: 1 step: 795, loss is 1.6883268\n",
      "epoch: 1 step: 796, loss is 1.3504122\n",
      "epoch: 1 step: 797, loss is 1.3532026\n",
      "epoch: 1 step: 798, loss is 1.454623\n",
      "epoch: 1 step: 799, loss is 1.4457095\n",
      "epoch: 1 step: 800, loss is 1.3849977\n",
      "epoch: 1 step: 801, loss is 1.3238533\n",
      "epoch: 1 step: 802, loss is 1.5689843\n",
      "epoch: 1 step: 803, loss is 1.4663016\n",
      "epoch: 1 step: 804, loss is 1.3114599\n",
      "epoch: 1 step: 805, loss is 1.7217902\n",
      "epoch: 1 step: 806, loss is 2.150619\n",
      "epoch: 1 step: 807, loss is 1.4854736\n",
      "epoch: 1 step: 808, loss is 1.4895873\n",
      "epoch: 1 step: 809, loss is 1.4489598\n",
      "epoch: 1 step: 810, loss is 1.4398962\n",
      "epoch: 1 step: 811, loss is 1.7397197\n",
      "epoch: 1 step: 812, loss is 1.7557591\n",
      "epoch: 1 step: 813, loss is 1.7226541\n",
      "epoch: 1 step: 814, loss is 1.8559726\n",
      "epoch: 1 step: 815, loss is 1.5679902\n",
      "epoch: 1 step: 816, loss is 1.3936771\n",
      "epoch: 1 step: 817, loss is 1.8254064\n",
      "epoch: 1 step: 818, loss is 1.7157422\n",
      "epoch: 1 step: 819, loss is 1.2469425\n",
      "epoch: 1 step: 820, loss is 1.5533998\n",
      "epoch: 1 step: 821, loss is 1.6872964\n",
      "epoch: 1 step: 822, loss is 1.252748\n",
      "epoch: 1 step: 823, loss is 1.7873213\n",
      "epoch: 1 step: 824, loss is 1.4486023\n",
      "epoch: 1 step: 825, loss is 1.7306702\n",
      "epoch: 1 step: 826, loss is 1.3872645\n",
      "epoch: 1 step: 827, loss is 1.4599519\n",
      "epoch: 1 step: 828, loss is 1.3569427\n",
      "epoch: 1 step: 829, loss is 1.8336968\n",
      "epoch: 1 step: 830, loss is 1.7688931\n",
      "epoch: 1 step: 831, loss is 1.5102602\n",
      "epoch: 1 step: 832, loss is 1.3600377\n",
      "epoch: 1 step: 833, loss is 1.3111826\n",
      "epoch: 1 step: 834, loss is 1.4624367\n",
      "epoch: 1 step: 835, loss is 1.3919764\n",
      "epoch: 1 step: 836, loss is 1.2693411\n",
      "epoch: 1 step: 837, loss is 1.708601\n",
      "epoch: 1 step: 838, loss is 1.4012693\n",
      "epoch: 1 step: 839, loss is 1.9653497\n",
      "epoch: 1 step: 840, loss is 1.4357191\n",
      "epoch: 1 step: 841, loss is 1.2059256\n",
      "epoch: 1 step: 842, loss is 1.2293736\n",
      "epoch: 1 step: 843, loss is 1.349649\n",
      "epoch: 1 step: 844, loss is 1.208581\n",
      "epoch: 1 step: 845, loss is 1.3696873\n",
      "epoch: 1 step: 846, loss is 1.5907419\n",
      "epoch: 1 step: 847, loss is 1.591914\n",
      "epoch: 1 step: 848, loss is 1.8846754\n",
      "epoch: 1 step: 849, loss is 2.0746405\n",
      "epoch: 1 step: 850, loss is 1.361839\n",
      "epoch: 1 step: 851, loss is 1.5705714\n",
      "epoch: 1 step: 852, loss is 1.3171859\n",
      "epoch: 1 step: 853, loss is 1.564649\n",
      "epoch: 1 step: 854, loss is 1.6219655\n",
      "epoch: 1 step: 855, loss is 1.331484\n",
      "epoch: 1 step: 856, loss is 1.5024288\n",
      "epoch: 1 step: 857, loss is 1.4688716\n",
      "epoch: 1 step: 858, loss is 1.6462464\n",
      "epoch: 1 step: 859, loss is 1.3438923\n",
      "epoch: 1 step: 860, loss is 1.711892\n",
      "epoch: 1 step: 861, loss is 1.6997507\n",
      "epoch: 1 step: 862, loss is 1.435498\n",
      "epoch: 1 step: 863, loss is 1.1307099\n",
      "epoch: 1 step: 864, loss is 1.6269209\n",
      "epoch: 1 step: 865, loss is 1.4088912\n",
      "epoch: 1 step: 866, loss is 1.4776165\n",
      "epoch: 1 step: 867, loss is 1.6498933\n",
      "epoch: 1 step: 868, loss is 1.3942738\n",
      "epoch: 1 step: 869, loss is 1.4923183\n",
      "epoch: 1 step: 870, loss is 1.8891602\n",
      "epoch: 1 step: 871, loss is 1.5044785\n",
      "epoch: 1 step: 872, loss is 1.1160651\n",
      "epoch: 1 step: 873, loss is 1.3085399\n",
      "epoch: 1 step: 874, loss is 1.6242464\n",
      "epoch: 1 step: 875, loss is 1.6066344\n",
      "epoch: 1 step: 876, loss is 1.8766729\n",
      "epoch: 1 step: 877, loss is 1.4814401\n",
      "epoch: 1 step: 878, loss is 1.4490204\n",
      "epoch: 1 step: 879, loss is 1.3142712\n",
      "epoch: 1 step: 880, loss is 1.6190886\n",
      "epoch: 1 step: 881, loss is 1.5608518\n",
      "epoch: 1 step: 882, loss is 1.3481792\n",
      "epoch: 1 step: 883, loss is 1.4756972\n",
      "epoch: 1 step: 884, loss is 1.6390324\n",
      "epoch: 1 step: 885, loss is 1.588571\n",
      "epoch: 1 step: 886, loss is 1.3685479\n",
      "epoch: 1 step: 887, loss is 1.2873362\n",
      "epoch: 1 step: 888, loss is 1.489894\n",
      "epoch: 1 step: 889, loss is 1.3374528\n",
      "epoch: 1 step: 890, loss is 1.4756087\n",
      "epoch: 1 step: 891, loss is 1.480512\n",
      "epoch: 1 step: 892, loss is 1.6492411\n",
      "epoch: 1 step: 893, loss is 1.2458729\n",
      "epoch: 1 step: 894, loss is 1.2684475\n",
      "epoch: 1 step: 895, loss is 1.3896431\n",
      "epoch: 1 step: 896, loss is 0.9824625\n",
      "epoch: 1 step: 897, loss is 1.5819974\n",
      "epoch: 1 step: 898, loss is 1.6504169\n",
      "epoch: 1 step: 899, loss is 1.5496949\n",
      "epoch: 1 step: 900, loss is 1.7829409\n",
      "epoch: 1 step: 901, loss is 1.5067428\n",
      "epoch: 1 step: 902, loss is 1.6382723\n",
      "epoch: 1 step: 903, loss is 1.259472\n",
      "epoch: 1 step: 904, loss is 1.3952217\n",
      "epoch: 1 step: 905, loss is 1.4132004\n",
      "epoch: 1 step: 906, loss is 1.5740336\n",
      "epoch: 1 step: 907, loss is 1.6748351\n",
      "epoch: 1 step: 908, loss is 1.4785639\n",
      "epoch: 1 step: 909, loss is 1.4115045\n",
      "epoch: 1 step: 910, loss is 1.1989037\n",
      "epoch: 1 step: 911, loss is 1.4481242\n",
      "epoch: 1 step: 912, loss is 1.2673893\n",
      "epoch: 1 step: 913, loss is 1.5549996\n",
      "epoch: 1 step: 914, loss is 1.5011615\n",
      "epoch: 1 step: 915, loss is 1.3693105\n",
      "epoch: 1 step: 916, loss is 1.7101324\n",
      "epoch: 1 step: 917, loss is 1.9230043\n",
      "epoch: 1 step: 918, loss is 1.4434799\n",
      "epoch: 1 step: 919, loss is 1.3335626\n",
      "epoch: 1 step: 920, loss is 1.6657344\n",
      "epoch: 1 step: 921, loss is 1.3825636\n",
      "epoch: 1 step: 922, loss is 1.33219\n",
      "epoch: 1 step: 923, loss is 1.2195286\n",
      "epoch: 1 step: 924, loss is 1.6127725\n",
      "epoch: 1 step: 925, loss is 1.5446659\n",
      "epoch: 1 step: 926, loss is 1.5270774\n",
      "epoch: 1 step: 927, loss is 1.2580628\n",
      "epoch: 1 step: 928, loss is 1.7041657\n",
      "epoch: 1 step: 929, loss is 1.5758098\n",
      "epoch: 1 step: 930, loss is 1.7102484\n",
      "epoch: 1 step: 931, loss is 1.3994572\n",
      "epoch: 1 step: 932, loss is 1.9130007\n",
      "epoch: 1 step: 933, loss is 1.4609923\n",
      "epoch: 1 step: 934, loss is 1.6357304\n",
      "epoch: 1 step: 935, loss is 1.5915947\n",
      "epoch: 1 step: 936, loss is 1.7038503\n",
      "epoch: 1 step: 937, loss is 1.7932838\n",
      "epoch: 1 step: 938, loss is 1.8098928\n",
      "epoch: 1 step: 939, loss is 1.5389311\n",
      "epoch: 1 step: 940, loss is 1.3928896\n",
      "epoch: 1 step: 941, loss is 1.5327642\n",
      "epoch: 1 step: 942, loss is 1.3825339\n",
      "epoch: 1 step: 943, loss is 1.5264933\n",
      "epoch: 1 step: 944, loss is 1.7062039\n",
      "epoch: 1 step: 945, loss is 1.444091\n",
      "epoch: 1 step: 946, loss is 1.2777262\n",
      "epoch: 1 step: 947, loss is 1.5758573\n",
      "epoch: 1 step: 948, loss is 1.3683653\n",
      "epoch: 1 step: 949, loss is 1.3301063\n",
      "epoch: 1 step: 950, loss is 1.4986534\n",
      "epoch: 1 step: 951, loss is 1.2256868\n",
      "epoch: 1 step: 952, loss is 1.2644176\n",
      "epoch: 1 step: 953, loss is 1.314643\n",
      "epoch: 1 step: 954, loss is 1.2385875\n",
      "epoch: 1 step: 955, loss is 1.4542121\n",
      "epoch: 1 step: 956, loss is 1.4822793\n",
      "epoch: 1 step: 957, loss is 1.5829598\n",
      "epoch: 1 step: 958, loss is 1.6081673\n",
      "epoch: 1 step: 959, loss is 1.7712667\n",
      "epoch: 1 step: 960, loss is 1.6777483\n",
      "epoch: 1 step: 961, loss is 1.334409\n",
      "epoch: 1 step: 962, loss is 1.441094\n",
      "epoch: 1 step: 963, loss is 1.4229996\n",
      "epoch: 1 step: 964, loss is 1.8280238\n",
      "epoch: 1 step: 965, loss is 1.7353162\n",
      "epoch: 1 step: 966, loss is 1.4416665\n",
      "epoch: 1 step: 967, loss is 1.4918355\n",
      "epoch: 1 step: 968, loss is 1.4341437\n",
      "epoch: 1 step: 969, loss is 1.3339396\n",
      "epoch: 1 step: 970, loss is 1.3031428\n",
      "epoch: 1 step: 971, loss is 1.4451116\n",
      "epoch: 1 step: 972, loss is 1.6559064\n",
      "epoch: 1 step: 973, loss is 1.6111383\n",
      "epoch: 1 step: 974, loss is 1.3594902\n",
      "epoch: 1 step: 975, loss is 1.5005916\n",
      "epoch: 1 step: 976, loss is 1.4652009\n",
      "epoch: 1 step: 977, loss is 1.3031411\n",
      "epoch: 1 step: 978, loss is 1.4256244\n",
      "epoch: 1 step: 979, loss is 1.3818803\n",
      "epoch: 1 step: 980, loss is 1.42957\n",
      "epoch: 1 step: 981, loss is 1.3557397\n",
      "epoch: 1 step: 982, loss is 1.7601635\n",
      "epoch: 1 step: 983, loss is 1.4695306\n",
      "epoch: 1 step: 984, loss is 1.6036133\n",
      "epoch: 1 step: 985, loss is 1.7831589\n",
      "epoch: 1 step: 986, loss is 1.7448659\n",
      "epoch: 1 step: 987, loss is 1.72014\n",
      "epoch: 1 step: 988, loss is 1.5673283\n",
      "epoch: 1 step: 989, loss is 1.4096134\n",
      "epoch: 1 step: 990, loss is 1.7022134\n",
      "epoch: 1 step: 991, loss is 1.2921054\n",
      "epoch: 1 step: 992, loss is 1.1905224\n",
      "epoch: 1 step: 993, loss is 1.395827\n",
      "epoch: 1 step: 994, loss is 1.4777821\n",
      "epoch: 1 step: 995, loss is 1.7273118\n",
      "epoch: 1 step: 996, loss is 1.3744962\n",
      "epoch: 1 step: 997, loss is 1.4508905\n",
      "epoch: 1 step: 998, loss is 1.2951413\n",
      "epoch: 1 step: 999, loss is 1.5430012\n",
      "epoch: 1 step: 1000, loss is 1.3458233\n",
      "epoch: 1 step: 1001, loss is 1.2384318\n",
      "epoch: 1 step: 1002, loss is 1.2113471\n",
      "epoch: 1 step: 1003, loss is 1.5085616\n",
      "epoch: 1 step: 1004, loss is 1.1960477\n",
      "epoch: 1 step: 1005, loss is 1.2913035\n",
      "epoch: 1 step: 1006, loss is 1.398333\n",
      "epoch: 1 step: 1007, loss is 1.4717351\n",
      "epoch: 1 step: 1008, loss is 1.5024669\n",
      "epoch: 1 step: 1009, loss is 1.4483734\n",
      "epoch: 1 step: 1010, loss is 1.2113318\n",
      "epoch: 1 step: 1011, loss is 1.4121776\n",
      "epoch: 1 step: 1012, loss is 1.8839082\n",
      "epoch: 1 step: 1013, loss is 1.5584109\n",
      "epoch: 1 step: 1014, loss is 1.2443333\n",
      "epoch: 1 step: 1015, loss is 1.3696373\n",
      "epoch: 1 step: 1016, loss is 1.3069987\n",
      "epoch: 1 step: 1017, loss is 1.5653514\n",
      "epoch: 1 step: 1018, loss is 1.3767468\n",
      "epoch: 1 step: 1019, loss is 1.5904739\n",
      "epoch: 1 step: 1020, loss is 1.4620854\n",
      "epoch: 1 step: 1021, loss is 1.2712709\n",
      "epoch: 1 step: 1022, loss is 1.797658\n",
      "epoch: 1 step: 1023, loss is 1.1521807\n",
      "epoch: 1 step: 1024, loss is 1.30248\n",
      "epoch: 1 step: 1025, loss is 1.8211626\n",
      "epoch: 1 step: 1026, loss is 1.3144404\n",
      "epoch: 1 step: 1027, loss is 1.44165\n",
      "epoch: 1 step: 1028, loss is 1.6172012\n",
      "epoch: 1 step: 1029, loss is 1.4775047\n",
      "epoch: 1 step: 1030, loss is 1.4132493\n",
      "epoch: 1 step: 1031, loss is 1.5251741\n",
      "epoch: 1 step: 1032, loss is 1.7366474\n",
      "epoch: 1 step: 1033, loss is 1.397927\n",
      "epoch: 1 step: 1034, loss is 1.5982419\n",
      "epoch: 1 step: 1035, loss is 1.6258199\n",
      "epoch: 1 step: 1036, loss is 1.4734569\n",
      "epoch: 1 step: 1037, loss is 1.8076224\n",
      "epoch: 1 step: 1038, loss is 1.4513502\n",
      "epoch: 1 step: 1039, loss is 1.2945876\n",
      "epoch: 1 step: 1040, loss is 1.276476\n",
      "epoch: 1 step: 1041, loss is 1.4204063\n",
      "epoch: 1 step: 1042, loss is 1.0693245\n",
      "epoch: 1 step: 1043, loss is 1.2306741\n",
      "epoch: 1 step: 1044, loss is 1.456258\n",
      "epoch: 1 step: 1045, loss is 1.5089378\n",
      "epoch: 1 step: 1046, loss is 1.1161971\n",
      "epoch: 1 step: 1047, loss is 1.4507568\n",
      "epoch: 1 step: 1048, loss is 1.0961611\n",
      "epoch: 1 step: 1049, loss is 1.065663\n",
      "epoch: 1 step: 1050, loss is 1.706359\n",
      "epoch: 1 step: 1051, loss is 1.970385\n",
      "epoch: 1 step: 1052, loss is 1.0644354\n",
      "epoch: 1 step: 1053, loss is 1.443713\n",
      "epoch: 1 step: 1054, loss is 1.6061213\n",
      "epoch: 1 step: 1055, loss is 1.4737896\n",
      "epoch: 1 step: 1056, loss is 1.7225637\n",
      "epoch: 1 step: 1057, loss is 1.8384788\n",
      "epoch: 1 step: 1058, loss is 1.4087067\n",
      "epoch: 1 step: 1059, loss is 1.3848275\n",
      "epoch: 1 step: 1060, loss is 1.6772137\n",
      "epoch: 1 step: 1061, loss is 1.4534919\n",
      "epoch: 1 step: 1062, loss is 1.49002\n",
      "epoch: 1 step: 1063, loss is 1.9051793\n",
      "epoch: 1 step: 1064, loss is 1.5644413\n",
      "epoch: 1 step: 1065, loss is 1.6646233\n",
      "epoch: 1 step: 1066, loss is 1.4511542\n",
      "epoch: 1 step: 1067, loss is 1.5159\n",
      "epoch: 1 step: 1068, loss is 1.445998\n",
      "epoch: 1 step: 1069, loss is 1.5863774\n",
      "epoch: 1 step: 1070, loss is 1.4788216\n",
      "epoch: 1 step: 1071, loss is 1.6058425\n",
      "epoch: 1 step: 1072, loss is 1.4761248\n",
      "epoch: 1 step: 1073, loss is 1.5854299\n",
      "epoch: 1 step: 1074, loss is 1.3916101\n",
      "epoch: 1 step: 1075, loss is 1.3474908\n",
      "epoch: 1 step: 1076, loss is 1.1875912\n",
      "epoch: 1 step: 1077, loss is 1.4161346\n",
      "epoch: 1 step: 1078, loss is 1.556757\n",
      "epoch: 1 step: 1079, loss is 1.4876155\n",
      "epoch: 1 step: 1080, loss is 1.352218\n",
      "epoch: 1 step: 1081, loss is 1.3532165\n",
      "epoch: 1 step: 1082, loss is 1.372335\n",
      "epoch: 1 step: 1083, loss is 1.5183724\n",
      "epoch: 1 step: 1084, loss is 1.549267\n",
      "epoch: 1 step: 1085, loss is 1.8480995\n",
      "epoch: 1 step: 1086, loss is 1.6611751\n",
      "epoch: 1 step: 1087, loss is 1.6881928\n",
      "epoch: 1 step: 1088, loss is 1.6468863\n",
      "epoch: 1 step: 1089, loss is 1.3608321\n",
      "epoch: 1 step: 1090, loss is 1.5937457\n",
      "epoch: 1 step: 1091, loss is 1.6284924\n",
      "epoch: 1 step: 1092, loss is 1.2603408\n",
      "epoch: 1 step: 1093, loss is 1.7778769\n",
      "epoch: 1 step: 1094, loss is 1.4931196\n",
      "epoch: 1 step: 1095, loss is 1.5560136\n",
      "epoch: 1 step: 1096, loss is 1.4003458\n",
      "epoch: 1 step: 1097, loss is 1.6478119\n",
      "epoch: 1 step: 1098, loss is 1.6385885\n",
      "epoch: 1 step: 1099, loss is 1.7106556\n",
      "epoch: 1 step: 1100, loss is 1.6742948\n",
      "epoch: 1 step: 1101, loss is 1.5681523\n",
      "epoch: 1 step: 1102, loss is 1.4641924\n",
      "epoch: 1 step: 1103, loss is 1.687712\n",
      "epoch: 1 step: 1104, loss is 1.6642405\n",
      "epoch: 1 step: 1105, loss is 1.451117\n",
      "epoch: 1 step: 1106, loss is 1.5841445\n",
      "epoch: 1 step: 1107, loss is 1.808717\n",
      "epoch: 1 step: 1108, loss is 1.5405357\n",
      "epoch: 1 step: 1109, loss is 1.3485787\n",
      "epoch: 1 step: 1110, loss is 1.4445091\n",
      "epoch: 1 step: 1111, loss is 1.2857138\n",
      "epoch: 1 step: 1112, loss is 1.4590292\n",
      "epoch: 1 step: 1113, loss is 1.3071064\n",
      "epoch: 1 step: 1114, loss is 1.2904329\n",
      "epoch: 1 step: 1115, loss is 1.4730778\n",
      "epoch: 1 step: 1116, loss is 1.5308706\n",
      "epoch: 1 step: 1117, loss is 1.5450733\n",
      "epoch: 1 step: 1118, loss is 1.74954\n",
      "epoch: 1 step: 1119, loss is 1.3807975\n",
      "epoch: 1 step: 1120, loss is 1.5011597\n",
      "epoch: 1 step: 1121, loss is 1.5826092\n",
      "epoch: 1 step: 1122, loss is 1.5316843\n",
      "epoch: 1 step: 1123, loss is 1.6264173\n",
      "epoch: 1 step: 1124, loss is 1.5174432\n",
      "epoch: 1 step: 1125, loss is 1.0874186\n",
      "epoch: 1 step: 1126, loss is 1.380205\n",
      "epoch: 1 step: 1127, loss is 1.5872867\n",
      "epoch: 1 step: 1128, loss is 1.6201686\n",
      "epoch: 1 step: 1129, loss is 1.778029\n",
      "epoch: 1 step: 1130, loss is 1.4600217\n",
      "epoch: 1 step: 1131, loss is 1.5347134\n",
      "epoch: 1 step: 1132, loss is 1.4027649\n",
      "epoch: 1 step: 1133, loss is 0.9226604\n",
      "epoch: 1 step: 1134, loss is 1.3055605\n",
      "epoch: 1 step: 1135, loss is 1.0442026\n",
      "epoch: 1 step: 1136, loss is 1.1808587\n",
      "epoch: 1 step: 1137, loss is 1.377135\n",
      "epoch: 1 step: 1138, loss is 1.6877035\n",
      "epoch: 1 step: 1139, loss is 1.2983209\n",
      "epoch: 1 step: 1140, loss is 1.3872854\n",
      "epoch: 1 step: 1141, loss is 1.3618586\n",
      "epoch: 1 step: 1142, loss is 1.387122\n",
      "epoch: 1 step: 1143, loss is 1.2764407\n",
      "epoch: 1 step: 1144, loss is 1.3603711\n",
      "epoch: 1 step: 1145, loss is 1.2283581\n",
      "epoch: 1 step: 1146, loss is 1.2546368\n",
      "epoch: 1 step: 1147, loss is 1.6764634\n",
      "epoch: 1 step: 1148, loss is 1.6907865\n",
      "epoch: 1 step: 1149, loss is 1.2837692\n",
      "epoch: 1 step: 1150, loss is 1.8318495\n",
      "epoch: 1 step: 1151, loss is 1.2147037\n",
      "epoch: 1 step: 1152, loss is 1.4042485\n",
      "epoch: 1 step: 1153, loss is 1.4859236\n",
      "epoch: 1 step: 1154, loss is 1.5518248\n",
      "epoch: 1 step: 1155, loss is 1.3974032\n",
      "epoch: 1 step: 1156, loss is 1.6316731\n",
      "epoch: 1 step: 1157, loss is 1.2188969\n",
      "epoch: 1 step: 1158, loss is 1.6083004\n",
      "epoch: 1 step: 1159, loss is 1.5649402\n",
      "epoch: 1 step: 1160, loss is 1.4469228\n",
      "epoch: 1 step: 1161, loss is 1.216704\n",
      "epoch: 1 step: 1162, loss is 1.6067665\n",
      "epoch: 1 step: 1163, loss is 1.2443733\n",
      "epoch: 1 step: 1164, loss is 1.4530845\n",
      "epoch: 1 step: 1165, loss is 1.2771873\n",
      "epoch: 1 step: 1166, loss is 1.5217347\n",
      "epoch: 1 step: 1167, loss is 1.0678883\n",
      "epoch: 1 step: 1168, loss is 1.2467906\n",
      "epoch: 1 step: 1169, loss is 1.2204192\n",
      "epoch: 1 step: 1170, loss is 1.2976882\n",
      "epoch: 1 step: 1171, loss is 1.1634655\n",
      "epoch: 1 step: 1172, loss is 1.8094522\n",
      "epoch: 1 step: 1173, loss is 1.4009162\n",
      "epoch: 1 step: 1174, loss is 1.2165064\n",
      "epoch: 1 step: 1175, loss is 1.4358938\n",
      "epoch: 1 step: 1176, loss is 1.3563329\n",
      "epoch: 1 step: 1177, loss is 1.3418909\n",
      "epoch: 1 step: 1178, loss is 1.588388\n",
      "epoch: 1 step: 1179, loss is 1.5001723\n",
      "epoch: 1 step: 1180, loss is 1.0620933\n",
      "epoch: 1 step: 1181, loss is 1.3127391\n",
      "epoch: 1 step: 1182, loss is 1.3291957\n",
      "epoch: 1 step: 1183, loss is 1.6128495\n",
      "epoch: 1 step: 1184, loss is 1.3058003\n",
      "epoch: 1 step: 1185, loss is 1.7343352\n",
      "epoch: 1 step: 1186, loss is 1.519294\n",
      "epoch: 1 step: 1187, loss is 1.4770554\n",
      "epoch: 1 step: 1188, loss is 1.409963\n",
      "epoch: 1 step: 1189, loss is 1.2188413\n",
      "epoch: 1 step: 1190, loss is 1.4385418\n",
      "epoch: 1 step: 1191, loss is 1.3666362\n",
      "epoch: 1 step: 1192, loss is 1.2568272\n",
      "epoch: 1 step: 1193, loss is 1.386971\n",
      "epoch: 1 step: 1194, loss is 1.2799462\n",
      "epoch: 1 step: 1195, loss is 1.4655839\n",
      "epoch: 1 step: 1196, loss is 1.4127765\n",
      "epoch: 1 step: 1197, loss is 1.3922477\n",
      "epoch: 1 step: 1198, loss is 1.3057139\n",
      "epoch: 1 step: 1199, loss is 1.5289812\n",
      "epoch: 1 step: 1200, loss is 1.6930197\n",
      "epoch: 1 step: 1201, loss is 1.2226915\n",
      "epoch: 1 step: 1202, loss is 1.2879531\n",
      "epoch: 1 step: 1203, loss is 1.3844141\n",
      "epoch: 1 step: 1204, loss is 1.4613597\n",
      "epoch: 1 step: 1205, loss is 1.1551809\n",
      "epoch: 1 step: 1206, loss is 1.512569\n",
      "epoch: 1 step: 1207, loss is 1.4822345\n",
      "epoch: 1 step: 1208, loss is 1.5110956\n",
      "epoch: 1 step: 1209, loss is 1.5963727\n",
      "epoch: 1 step: 1210, loss is 1.4299576\n",
      "epoch: 1 step: 1211, loss is 1.231869\n",
      "epoch: 1 step: 1212, loss is 1.456958\n",
      "epoch: 1 step: 1213, loss is 1.5511466\n",
      "epoch: 1 step: 1214, loss is 1.6488792\n",
      "epoch: 1 step: 1215, loss is 1.40817\n",
      "epoch: 1 step: 1216, loss is 1.2532108\n",
      "epoch: 1 step: 1217, loss is 1.873541\n",
      "epoch: 1 step: 1218, loss is 1.7762179\n",
      "epoch: 1 step: 1219, loss is 1.5154253\n",
      "epoch: 1 step: 1220, loss is 1.7265729\n",
      "epoch: 1 step: 1221, loss is 1.3876758\n",
      "epoch: 1 step: 1222, loss is 1.6509043\n",
      "epoch: 1 step: 1223, loss is 1.4896164\n",
      "epoch: 1 step: 1224, loss is 1.3151089\n",
      "epoch: 1 step: 1225, loss is 1.4151506\n",
      "epoch: 1 step: 1226, loss is 1.1820964\n",
      "epoch: 1 step: 1227, loss is 1.4543096\n",
      "epoch: 1 step: 1228, loss is 1.5514011\n",
      "epoch: 1 step: 1229, loss is 1.5902969\n",
      "epoch: 1 step: 1230, loss is 1.2839022\n",
      "epoch: 1 step: 1231, loss is 1.8519837\n",
      "epoch: 1 step: 1232, loss is 1.4050322\n",
      "epoch: 1 step: 1233, loss is 1.1392316\n",
      "epoch: 1 step: 1234, loss is 1.4565428\n",
      "epoch: 1 step: 1235, loss is 1.4197584\n",
      "epoch: 1 step: 1236, loss is 1.7372535\n",
      "epoch: 1 step: 1237, loss is 1.5060354\n",
      "epoch: 1 step: 1238, loss is 1.5954545\n",
      "epoch: 1 step: 1239, loss is 1.3347198\n",
      "epoch: 1 step: 1240, loss is 1.5955516\n",
      "epoch: 1 step: 1241, loss is 1.3998997\n",
      "epoch: 1 step: 1242, loss is 1.426598\n",
      "epoch: 1 step: 1243, loss is 1.5989623\n",
      "epoch: 1 step: 1244, loss is 1.3325846\n",
      "epoch: 1 step: 1245, loss is 1.3977913\n",
      "epoch: 1 step: 1246, loss is 1.7152063\n",
      "epoch: 1 step: 1247, loss is 1.5539844\n",
      "epoch: 1 step: 1248, loss is 1.2404729\n",
      "epoch: 1 step: 1249, loss is 1.4931751\n",
      "epoch: 1 step: 1250, loss is 1.2920052\n",
      "epoch: 1 step: 1251, loss is 1.1780192\n",
      "epoch: 1 step: 1252, loss is 1.2442629\n",
      "epoch: 1 step: 1253, loss is 1.1723812\n",
      "epoch: 1 step: 1254, loss is 1.4218501\n",
      "epoch: 1 step: 1255, loss is 1.3327094\n",
      "epoch: 1 step: 1256, loss is 1.4524192\n",
      "epoch: 1 step: 1257, loss is 1.3477719\n",
      "epoch: 1 step: 1258, loss is 1.1938019\n",
      "epoch: 1 step: 1259, loss is 1.9049801\n",
      "epoch: 1 step: 1260, loss is 1.4126456\n",
      "epoch: 1 step: 1261, loss is 1.4140674\n",
      "epoch: 1 step: 1262, loss is 0.9267416\n",
      "epoch: 1 step: 1263, loss is 1.0137879\n",
      "epoch: 1 step: 1264, loss is 1.2279176\n",
      "epoch: 1 step: 1265, loss is 1.567262\n",
      "epoch: 1 step: 1266, loss is 1.2285054\n",
      "epoch: 1 step: 1267, loss is 1.1534625\n",
      "epoch: 1 step: 1268, loss is 1.2858497\n",
      "epoch: 1 step: 1269, loss is 1.5605412\n",
      "epoch: 1 step: 1270, loss is 1.56226\n",
      "epoch: 1 step: 1271, loss is 1.3020967\n",
      "epoch: 1 step: 1272, loss is 1.381936\n",
      "epoch: 1 step: 1273, loss is 1.3369573\n",
      "epoch: 1 step: 1274, loss is 1.301429\n",
      "epoch: 1 step: 1275, loss is 1.5574522\n",
      "epoch: 1 step: 1276, loss is 1.4781239\n",
      "epoch: 1 step: 1277, loss is 1.2316637\n",
      "epoch: 1 step: 1278, loss is 1.3016093\n",
      "epoch: 1 step: 1279, loss is 1.2138153\n",
      "epoch: 1 step: 1280, loss is 1.3785521\n",
      "epoch: 1 step: 1281, loss is 1.4024807\n",
      "epoch: 1 step: 1282, loss is 1.3555727\n",
      "epoch: 1 step: 1283, loss is 1.5651853\n",
      "epoch: 1 step: 1284, loss is 1.4016315\n",
      "epoch: 1 step: 1285, loss is 1.2198036\n",
      "epoch: 1 step: 1286, loss is 1.2754571\n",
      "epoch: 1 step: 1287, loss is 1.8121784\n",
      "epoch: 1 step: 1288, loss is 1.8557888\n",
      "epoch: 1 step: 1289, loss is 1.0378834\n",
      "epoch: 1 step: 1290, loss is 1.331394\n",
      "epoch: 1 step: 1291, loss is 1.6162623\n",
      "epoch: 1 step: 1292, loss is 1.3346548\n",
      "epoch: 1 step: 1293, loss is 1.3188528\n",
      "epoch: 1 step: 1294, loss is 1.365557\n",
      "epoch: 1 step: 1295, loss is 1.4577495\n",
      "epoch: 1 step: 1296, loss is 1.4080956\n",
      "epoch: 1 step: 1297, loss is 1.3881235\n",
      "epoch: 1 step: 1298, loss is 1.2581172\n",
      "epoch: 1 step: 1299, loss is 1.8411022\n",
      "epoch: 1 step: 1300, loss is 1.8309569\n",
      "epoch: 1 step: 1301, loss is 1.714699\n",
      "epoch: 1 step: 1302, loss is 1.5230201\n",
      "epoch: 1 step: 1303, loss is 1.4072515\n",
      "epoch: 1 step: 1304, loss is 1.5895238\n",
      "epoch: 1 step: 1305, loss is 1.4734458\n",
      "epoch: 1 step: 1306, loss is 1.6693139\n",
      "epoch: 1 step: 1307, loss is 1.3508021\n",
      "epoch: 1 step: 1308, loss is 1.4020185\n",
      "epoch: 1 step: 1309, loss is 1.3456501\n",
      "epoch: 1 step: 1310, loss is 1.4827319\n",
      "epoch: 1 step: 1311, loss is 1.4492197\n",
      "epoch: 1 step: 1312, loss is 1.37158\n",
      "epoch: 1 step: 1313, loss is 1.1524137\n",
      "epoch: 1 step: 1314, loss is 1.2289648\n",
      "epoch: 1 step: 1315, loss is 1.4254416\n",
      "epoch: 1 step: 1316, loss is 1.1410562\n",
      "epoch: 1 step: 1317, loss is 1.4644802\n",
      "epoch: 1 step: 1318, loss is 1.271628\n",
      "epoch: 1 step: 1319, loss is 1.3115672\n",
      "epoch: 1 step: 1320, loss is 1.633274\n",
      "epoch: 1 step: 1321, loss is 1.465391\n",
      "epoch: 1 step: 1322, loss is 1.2472517\n",
      "epoch: 1 step: 1323, loss is 1.2818749\n",
      "epoch: 1 step: 1324, loss is 1.4363542\n",
      "epoch: 1 step: 1325, loss is 1.4103538\n",
      "epoch: 1 step: 1326, loss is 1.214358\n",
      "epoch: 1 step: 1327, loss is 1.0657078\n",
      "epoch: 1 step: 1328, loss is 1.3454388\n",
      "epoch: 1 step: 1329, loss is 1.2308755\n",
      "epoch: 1 step: 1330, loss is 0.98382366\n",
      "epoch: 1 step: 1331, loss is 1.3479354\n",
      "epoch: 1 step: 1332, loss is 1.2566881\n",
      "epoch: 1 step: 1333, loss is 1.1400638\n",
      "epoch: 1 step: 1334, loss is 1.2015454\n",
      "epoch: 1 step: 1335, loss is 1.3805159\n",
      "epoch: 1 step: 1336, loss is 1.4322407\n",
      "epoch: 1 step: 1337, loss is 1.8080709\n",
      "epoch: 1 step: 1338, loss is 1.5804207\n",
      "epoch: 1 step: 1339, loss is 1.9374752\n",
      "epoch: 1 step: 1340, loss is 1.3877329\n",
      "epoch: 1 step: 1341, loss is 1.5624046\n",
      "epoch: 1 step: 1342, loss is 1.5108775\n",
      "epoch: 1 step: 1343, loss is 1.4967742\n",
      "epoch: 1 step: 1344, loss is 1.3880903\n",
      "epoch: 1 step: 1345, loss is 1.4173851\n",
      "epoch: 1 step: 1346, loss is 1.3169347\n",
      "epoch: 1 step: 1347, loss is 1.2694649\n",
      "epoch: 1 step: 1348, loss is 1.5865494\n",
      "epoch: 1 step: 1349, loss is 1.5347471\n",
      "epoch: 1 step: 1350, loss is 1.4074612\n",
      "epoch: 1 step: 1351, loss is 1.280741\n",
      "epoch: 1 step: 1352, loss is 1.5769352\n",
      "epoch: 1 step: 1353, loss is 1.3056018\n",
      "epoch: 1 step: 1354, loss is 1.6951033\n",
      "epoch: 1 step: 1355, loss is 1.3846172\n",
      "epoch: 1 step: 1356, loss is 1.5667539\n",
      "epoch: 1 step: 1357, loss is 1.9832652\n",
      "epoch: 1 step: 1358, loss is 1.9354026\n",
      "epoch: 1 step: 1359, loss is 1.1657454\n",
      "epoch: 1 step: 1360, loss is 1.5539632\n",
      "epoch: 1 step: 1361, loss is 1.5683771\n",
      "epoch: 1 step: 1362, loss is 1.284573\n",
      "epoch: 1 step: 1363, loss is 1.3234037\n",
      "epoch: 1 step: 1364, loss is 1.0742408\n",
      "epoch: 1 step: 1365, loss is 1.4454209\n",
      "epoch: 1 step: 1366, loss is 1.3352131\n",
      "epoch: 1 step: 1367, loss is 1.085486\n",
      "epoch: 1 step: 1368, loss is 1.3065667\n",
      "epoch: 1 step: 1369, loss is 1.2004462\n",
      "epoch: 1 step: 1370, loss is 1.387144\n",
      "epoch: 1 step: 1371, loss is 1.6743219\n",
      "epoch: 1 step: 1372, loss is 1.0173322\n",
      "epoch: 1 step: 1373, loss is 1.3794339\n",
      "epoch: 1 step: 1374, loss is 1.513809\n",
      "epoch: 1 step: 1375, loss is 1.1939296\n",
      "epoch: 1 step: 1376, loss is 1.6330478\n",
      "epoch: 1 step: 1377, loss is 1.444007\n",
      "epoch: 1 step: 1378, loss is 1.5189459\n",
      "epoch: 1 step: 1379, loss is 1.3200353\n",
      "epoch: 1 step: 1380, loss is 1.4232177\n",
      "epoch: 1 step: 1381, loss is 1.4717697\n",
      "epoch: 1 step: 1382, loss is 1.4825989\n",
      "epoch: 1 step: 1383, loss is 1.4204884\n",
      "epoch: 1 step: 1384, loss is 0.89584345\n",
      "epoch: 1 step: 1385, loss is 1.4679415\n",
      "epoch: 1 step: 1386, loss is 1.4281197\n",
      "epoch: 1 step: 1387, loss is 1.4700894\n",
      "epoch: 1 step: 1388, loss is 1.3891941\n",
      "epoch: 1 step: 1389, loss is 1.4268277\n",
      "epoch: 1 step: 1390, loss is 1.2713712\n",
      "epoch: 1 step: 1391, loss is 1.5808033\n",
      "epoch: 1 step: 1392, loss is 1.7268285\n",
      "epoch: 1 step: 1393, loss is 1.6377437\n",
      "epoch: 1 step: 1394, loss is 1.4286336\n",
      "epoch: 1 step: 1395, loss is 1.3952024\n",
      "epoch: 1 step: 1396, loss is 1.2343594\n",
      "epoch: 1 step: 1397, loss is 1.2339226\n",
      "epoch: 1 step: 1398, loss is 1.3672574\n",
      "epoch: 1 step: 1399, loss is 1.3638191\n",
      "epoch: 1 step: 1400, loss is 1.1517045\n",
      "epoch: 1 step: 1401, loss is 1.1962482\n",
      "epoch: 1 step: 1402, loss is 0.8746574\n",
      "epoch: 1 step: 1403, loss is 1.3253422\n",
      "epoch: 1 step: 1404, loss is 1.7217339\n",
      "epoch: 1 step: 1405, loss is 1.9047134\n",
      "epoch: 1 step: 1406, loss is 1.4968197\n",
      "epoch: 1 step: 1407, loss is 1.2894192\n",
      "epoch: 1 step: 1408, loss is 1.4501648\n",
      "epoch: 1 step: 1409, loss is 1.0751973\n",
      "epoch: 1 step: 1410, loss is 1.2765019\n",
      "epoch: 1 step: 1411, loss is 1.481401\n",
      "epoch: 1 step: 1412, loss is 1.5736792\n",
      "epoch: 1 step: 1413, loss is 1.2979448\n",
      "epoch: 1 step: 1414, loss is 1.6369971\n",
      "epoch: 1 step: 1415, loss is 0.99241436\n",
      "epoch: 1 step: 1416, loss is 1.1919184\n",
      "epoch: 1 step: 1417, loss is 1.2248693\n",
      "epoch: 1 step: 1418, loss is 1.1922894\n",
      "epoch: 1 step: 1419, loss is 1.3063817\n",
      "epoch: 1 step: 1420, loss is 1.4154152\n",
      "epoch: 1 step: 1421, loss is 1.476606\n",
      "epoch: 1 step: 1422, loss is 1.4862953\n",
      "epoch: 1 step: 1423, loss is 1.2912722\n",
      "epoch: 1 step: 1424, loss is 1.1221293\n",
      "epoch: 1 step: 1425, loss is 1.1408395\n",
      "epoch: 1 step: 1426, loss is 0.98100686\n",
      "epoch: 1 step: 1427, loss is 1.3401616\n",
      "epoch: 1 step: 1428, loss is 1.5375205\n",
      "epoch: 1 step: 1429, loss is 1.221243\n",
      "epoch: 1 step: 1430, loss is 1.2295316\n",
      "epoch: 1 step: 1431, loss is 1.6262039\n",
      "epoch: 1 step: 1432, loss is 1.2541381\n",
      "epoch: 1 step: 1433, loss is 1.4378821\n",
      "epoch: 1 step: 1434, loss is 1.020244\n",
      "epoch: 1 step: 1435, loss is 1.3573643\n",
      "epoch: 1 step: 1436, loss is 1.5676556\n",
      "epoch: 1 step: 1437, loss is 1.3484217\n",
      "epoch: 1 step: 1438, loss is 1.4384725\n",
      "epoch: 1 step: 1439, loss is 1.6482326\n",
      "epoch: 1 step: 1440, loss is 1.6377412\n",
      "epoch: 1 step: 1441, loss is 1.340309\n",
      "epoch: 1 step: 1442, loss is 1.2418038\n",
      "epoch: 1 step: 1443, loss is 0.95474607\n",
      "epoch: 1 step: 1444, loss is 1.341735\n",
      "epoch: 1 step: 1445, loss is 1.1686387\n",
      "epoch: 1 step: 1446, loss is 1.4560174\n",
      "epoch: 1 step: 1447, loss is 1.3657631\n",
      "epoch: 1 step: 1448, loss is 1.5936688\n",
      "epoch: 1 step: 1449, loss is 1.1428368\n",
      "epoch: 1 step: 1450, loss is 1.5132302\n",
      "epoch: 1 step: 1451, loss is 1.8899034\n",
      "epoch: 1 step: 1452, loss is 1.3629072\n",
      "epoch: 1 step: 1453, loss is 1.7259948\n",
      "epoch: 1 step: 1454, loss is 0.9989042\n",
      "epoch: 1 step: 1455, loss is 1.5283035\n",
      "epoch: 1 step: 1456, loss is 1.3522403\n",
      "epoch: 1 step: 1457, loss is 1.3304385\n",
      "epoch: 1 step: 1458, loss is 1.2703035\n",
      "epoch: 1 step: 1459, loss is 1.4387391\n",
      "epoch: 1 step: 1460, loss is 1.585381\n",
      "epoch: 1 step: 1461, loss is 1.5175371\n",
      "epoch: 1 step: 1462, loss is 1.2679572\n",
      "epoch: 1 step: 1463, loss is 1.1932198\n",
      "epoch: 1 step: 1464, loss is 1.4526349\n",
      "epoch: 1 step: 1465, loss is 1.3521781\n",
      "epoch: 1 step: 1466, loss is 1.4213707\n",
      "epoch: 1 step: 1467, loss is 1.3279833\n",
      "epoch: 1 step: 1468, loss is 1.4212073\n",
      "epoch: 1 step: 1469, loss is 1.3685215\n",
      "epoch: 1 step: 1470, loss is 1.056026\n",
      "epoch: 1 step: 1471, loss is 1.7004448\n",
      "epoch: 1 step: 1472, loss is 1.4746413\n",
      "epoch: 1 step: 1473, loss is 1.1879407\n",
      "epoch: 1 step: 1474, loss is 1.4215626\n",
      "epoch: 1 step: 1475, loss is 1.3326164\n",
      "epoch: 1 step: 1476, loss is 1.2764688\n",
      "epoch: 1 step: 1477, loss is 1.235063\n",
      "epoch: 1 step: 1478, loss is 1.3711689\n",
      "epoch: 1 step: 1479, loss is 1.0656219\n",
      "epoch: 1 step: 1480, loss is 1.5059135\n",
      "epoch: 1 step: 1481, loss is 1.5025238\n",
      "epoch: 1 step: 1482, loss is 1.191954\n",
      "epoch: 1 step: 1483, loss is 1.3097191\n",
      "epoch: 1 step: 1484, loss is 1.5643984\n",
      "epoch: 1 step: 1485, loss is 1.4441489\n",
      "epoch: 1 step: 1486, loss is 1.2910705\n",
      "epoch: 1 step: 1487, loss is 1.329676\n",
      "epoch: 1 step: 1488, loss is 1.305565\n",
      "epoch: 1 step: 1489, loss is 1.2667482\n",
      "epoch: 1 step: 1490, loss is 1.4357529\n",
      "epoch: 1 step: 1491, loss is 1.4351262\n",
      "epoch: 1 step: 1492, loss is 1.5519452\n",
      "epoch: 1 step: 1493, loss is 1.6148419\n",
      "epoch: 1 step: 1494, loss is 1.3838167\n",
      "epoch: 1 step: 1495, loss is 1.0835602\n",
      "epoch: 1 step: 1496, loss is 1.4714947\n",
      "epoch: 1 step: 1497, loss is 1.4932758\n",
      "epoch: 1 step: 1498, loss is 1.4088761\n",
      "epoch: 1 step: 1499, loss is 1.1779108\n",
      "epoch: 1 step: 1500, loss is 1.3356653\n",
      "epoch: 1 step: 1501, loss is 1.1667678\n",
      "epoch: 1 step: 1502, loss is 1.3619515\n",
      "epoch: 1 step: 1503, loss is 1.1729198\n",
      "epoch: 1 step: 1504, loss is 1.3724389\n",
      "epoch: 1 step: 1505, loss is 1.378713\n",
      "epoch: 1 step: 1506, loss is 1.6108943\n",
      "epoch: 1 step: 1507, loss is 1.2038759\n",
      "epoch: 1 step: 1508, loss is 1.0019314\n",
      "epoch: 1 step: 1509, loss is 1.5403825\n",
      "epoch: 1 step: 1510, loss is 1.2990553\n",
      "epoch: 1 step: 1511, loss is 1.2233635\n",
      "epoch: 1 step: 1512, loss is 1.6010514\n",
      "epoch: 1 step: 1513, loss is 1.1111878\n",
      "epoch: 1 step: 1514, loss is 1.2424178\n",
      "epoch: 1 step: 1515, loss is 1.6371001\n",
      "epoch: 1 step: 1516, loss is 1.4470384\n",
      "epoch: 1 step: 1517, loss is 1.4446534\n",
      "epoch: 1 step: 1518, loss is 1.7958599\n",
      "epoch: 1 step: 1519, loss is 1.1920159\n",
      "epoch: 1 step: 1520, loss is 1.2338734\n",
      "epoch: 1 step: 1521, loss is 1.2094498\n",
      "epoch: 1 step: 1522, loss is 1.2534564\n",
      "epoch: 1 step: 1523, loss is 1.5118634\n",
      "epoch: 1 step: 1524, loss is 1.2127262\n",
      "epoch: 1 step: 1525, loss is 1.632968\n",
      "epoch: 1 step: 1526, loss is 1.5494295\n",
      "epoch: 1 step: 1527, loss is 1.2364469\n",
      "epoch: 1 step: 1528, loss is 1.4633615\n",
      "epoch: 1 step: 1529, loss is 1.3178356\n",
      "epoch: 1 step: 1530, loss is 1.267679\n",
      "epoch: 1 step: 1531, loss is 1.0471388\n",
      "epoch: 1 step: 1532, loss is 1.3325788\n",
      "epoch: 1 step: 1533, loss is 1.5127991\n",
      "epoch: 1 step: 1534, loss is 1.4434981\n",
      "epoch: 1 step: 1535, loss is 1.382639\n",
      "epoch: 1 step: 1536, loss is 1.386233\n",
      "epoch: 1 step: 1537, loss is 1.4834467\n",
      "epoch: 1 step: 1538, loss is 1.201293\n",
      "epoch: 1 step: 1539, loss is 1.0090594\n",
      "epoch: 1 step: 1540, loss is 1.4761021\n",
      "epoch: 1 step: 1541, loss is 1.2868114\n",
      "epoch: 1 step: 1542, loss is 1.5211657\n",
      "epoch: 1 step: 1543, loss is 1.4550169\n",
      "epoch: 1 step: 1544, loss is 1.3568703\n",
      "epoch: 1 step: 1545, loss is 1.6387975\n",
      "epoch: 1 step: 1546, loss is 1.40154\n",
      "epoch: 1 step: 1547, loss is 1.265563\n",
      "epoch: 1 step: 1548, loss is 1.1493036\n",
      "epoch: 1 step: 1549, loss is 1.3965443\n",
      "epoch: 1 step: 1550, loss is 1.2260574\n",
      "epoch: 1 step: 1551, loss is 1.405554\n",
      "epoch: 1 step: 1552, loss is 1.3950657\n",
      "epoch: 1 step: 1553, loss is 1.1155882\n",
      "epoch: 1 step: 1554, loss is 1.3864212\n",
      "epoch: 1 step: 1555, loss is 1.5486262\n",
      "epoch: 1 step: 1556, loss is 1.2139142\n",
      "epoch: 1 step: 1557, loss is 1.2126276\n",
      "epoch: 1 step: 1558, loss is 1.3534621\n",
      "epoch: 1 step: 1559, loss is 1.0595696\n",
      "epoch: 1 step: 1560, loss is 1.5318354\n",
      "epoch: 1 step: 1561, loss is 1.2908386\n",
      "epoch: 1 step: 1562, loss is 1.2111496\n",
      "epoch: 1 step: 1563, loss is 1.2746801\n",
      "epoch: 1 step: 1564, loss is 1.1969472\n",
      "epoch: 1 step: 1565, loss is 1.2909548\n",
      "epoch: 1 step: 1566, loss is 1.3206508\n",
      "epoch: 1 step: 1567, loss is 1.2737576\n",
      "epoch: 1 step: 1568, loss is 1.1236336\n",
      "epoch: 1 step: 1569, loss is 1.5247233\n",
      "epoch: 1 step: 1570, loss is 1.4617217\n",
      "epoch: 1 step: 1571, loss is 1.254712\n",
      "epoch: 1 step: 1572, loss is 1.163388\n",
      "epoch: 1 step: 1573, loss is 1.4513651\n",
      "epoch: 1 step: 1574, loss is 0.87491655\n",
      "epoch: 1 step: 1575, loss is 1.2863545\n",
      "epoch: 1 step: 1576, loss is 1.7287784\n",
      "epoch: 1 step: 1577, loss is 1.2927375\n",
      "epoch: 1 step: 1578, loss is 1.0263655\n",
      "epoch: 1 step: 1579, loss is 1.4174054\n",
      "epoch: 1 step: 1580, loss is 1.6491966\n",
      "epoch: 1 step: 1581, loss is 1.2692546\n",
      "epoch: 1 step: 1582, loss is 1.0746864\n",
      "epoch: 1 step: 1583, loss is 1.3121701\n",
      "epoch: 1 step: 1584, loss is 1.3790495\n",
      "epoch: 1 step: 1585, loss is 1.4588695\n",
      "epoch: 1 step: 1586, loss is 1.284449\n",
      "epoch: 1 step: 1587, loss is 1.1390896\n",
      "epoch: 1 step: 1588, loss is 1.1514502\n",
      "epoch: 1 step: 1589, loss is 1.1185704\n",
      "epoch: 1 step: 1590, loss is 1.314196\n",
      "epoch: 1 step: 1591, loss is 1.2521688\n",
      "epoch: 1 step: 1592, loss is 1.4555963\n",
      "epoch: 1 step: 1593, loss is 1.6243101\n",
      "epoch: 1 step: 1594, loss is 1.246873\n",
      "epoch: 1 step: 1595, loss is 1.2454405\n",
      "epoch: 1 step: 1596, loss is 1.2532523\n",
      "epoch: 1 step: 1597, loss is 1.1908793\n",
      "epoch: 1 step: 1598, loss is 1.2570715\n",
      "epoch: 1 step: 1599, loss is 1.2755541\n",
      "epoch: 1 step: 1600, loss is 1.2097416\n",
      "epoch: 1 step: 1601, loss is 1.9029777\n",
      "epoch: 1 step: 1602, loss is 1.38921\n",
      "epoch: 1 step: 1603, loss is 1.4121152\n",
      "epoch: 1 step: 1604, loss is 1.3351032\n",
      "epoch: 1 step: 1605, loss is 1.4970455\n",
      "epoch: 1 step: 1606, loss is 1.3750672\n",
      "epoch: 1 step: 1607, loss is 1.1596807\n",
      "epoch: 1 step: 1608, loss is 1.4559925\n",
      "epoch: 1 step: 1609, loss is 1.2220306\n",
      "epoch: 1 step: 1610, loss is 1.4199386\n",
      "epoch: 1 step: 1611, loss is 1.2942992\n",
      "epoch: 1 step: 1612, loss is 1.2519042\n",
      "epoch: 1 step: 1613, loss is 1.0503359\n",
      "epoch: 1 step: 1614, loss is 1.387565\n",
      "epoch: 1 step: 1615, loss is 1.4490733\n",
      "epoch: 1 step: 1616, loss is 0.9314971\n",
      "epoch: 1 step: 1617, loss is 1.3857126\n",
      "epoch: 1 step: 1618, loss is 1.5104352\n",
      "epoch: 1 step: 1619, loss is 1.588238\n",
      "epoch: 1 step: 1620, loss is 0.91168046\n",
      "epoch: 1 step: 1621, loss is 1.2739056\n",
      "epoch: 1 step: 1622, loss is 1.292427\n",
      "epoch: 1 step: 1623, loss is 1.282861\n",
      "epoch: 1 step: 1624, loss is 1.1957629\n",
      "epoch: 1 step: 1625, loss is 1.3128257\n",
      "epoch: 1 step: 1626, loss is 1.005061\n",
      "epoch: 1 step: 1627, loss is 1.0797701\n",
      "epoch: 1 step: 1628, loss is 1.2383028\n",
      "epoch: 1 step: 1629, loss is 1.3915912\n",
      "epoch: 1 step: 1630, loss is 1.2259561\n",
      "epoch: 1 step: 1631, loss is 1.0481151\n",
      "epoch: 1 step: 1632, loss is 1.1385162\n",
      "epoch: 1 step: 1633, loss is 1.0948018\n",
      "epoch: 1 step: 1634, loss is 1.6091529\n",
      "epoch: 1 step: 1635, loss is 1.2354981\n",
      "epoch: 1 step: 1636, loss is 0.9310678\n",
      "epoch: 1 step: 1637, loss is 1.331626\n",
      "epoch: 1 step: 1638, loss is 1.0510672\n",
      "epoch: 1 step: 1639, loss is 1.1921046\n",
      "epoch: 1 step: 1640, loss is 1.1785682\n",
      "epoch: 1 step: 1641, loss is 1.4149497\n",
      "epoch: 1 step: 1642, loss is 1.3950738\n",
      "epoch: 1 step: 1643, loss is 1.3353533\n",
      "epoch: 1 step: 1644, loss is 1.38123\n",
      "epoch: 1 step: 1645, loss is 1.6331719\n",
      "epoch: 1 step: 1646, loss is 1.1311626\n",
      "epoch: 1 step: 1647, loss is 1.4007181\n",
      "epoch: 1 step: 1648, loss is 1.2563858\n",
      "epoch: 1 step: 1649, loss is 1.2561718\n",
      "epoch: 1 step: 1650, loss is 1.2818148\n",
      "epoch: 1 step: 1651, loss is 1.5910724\n",
      "epoch: 1 step: 1652, loss is 1.58153\n",
      "epoch: 1 step: 1653, loss is 1.363528\n",
      "epoch: 1 step: 1654, loss is 1.1695031\n",
      "epoch: 1 step: 1655, loss is 1.2809095\n",
      "epoch: 1 step: 1656, loss is 1.5083903\n",
      "epoch: 1 step: 1657, loss is 1.2943311\n",
      "epoch: 1 step: 1658, loss is 1.2324029\n",
      "epoch: 1 step: 1659, loss is 1.1595927\n",
      "epoch: 1 step: 1660, loss is 1.3818189\n",
      "epoch: 1 step: 1661, loss is 1.4895738\n",
      "epoch: 1 step: 1662, loss is 1.3365142\n",
      "epoch: 1 step: 1663, loss is 1.3263732\n",
      "epoch: 1 step: 1664, loss is 1.5203621\n",
      "epoch: 1 step: 1665, loss is 1.3346051\n",
      "epoch: 1 step: 1666, loss is 1.1670208\n",
      "epoch: 1 step: 1667, loss is 1.210093\n",
      "epoch: 1 step: 1668, loss is 1.4484779\n",
      "epoch: 1 step: 1669, loss is 1.2592001\n",
      "epoch: 1 step: 1670, loss is 1.5291618\n",
      "epoch: 1 step: 1671, loss is 1.542413\n",
      "epoch: 1 step: 1672, loss is 1.3598502\n",
      "epoch: 1 step: 1673, loss is 1.3164884\n",
      "epoch: 1 step: 1674, loss is 1.2745708\n",
      "epoch: 1 step: 1675, loss is 1.4697601\n",
      "epoch: 1 step: 1676, loss is 1.1420311\n",
      "epoch: 1 step: 1677, loss is 1.3872001\n",
      "epoch: 1 step: 1678, loss is 1.0936844\n",
      "epoch: 1 step: 1679, loss is 1.3481436\n",
      "epoch: 1 step: 1680, loss is 1.4533681\n",
      "epoch: 1 step: 1681, loss is 1.0340636\n",
      "epoch: 1 step: 1682, loss is 1.6927943\n",
      "epoch: 1 step: 1683, loss is 1.4507469\n",
      "epoch: 1 step: 1684, loss is 1.2210361\n",
      "epoch: 1 step: 1685, loss is 1.1637756\n",
      "epoch: 1 step: 1686, loss is 1.405785\n",
      "epoch: 1 step: 1687, loss is 1.0806653\n",
      "epoch: 1 step: 1688, loss is 1.2110412\n",
      "epoch: 1 step: 1689, loss is 1.0937907\n",
      "epoch: 1 step: 1690, loss is 1.4258275\n",
      "epoch: 1 step: 1691, loss is 1.4618931\n",
      "epoch: 1 step: 1692, loss is 1.2483357\n",
      "epoch: 1 step: 1693, loss is 1.5734866\n",
      "epoch: 1 step: 1694, loss is 1.1403701\n",
      "epoch: 1 step: 1695, loss is 1.4204731\n",
      "epoch: 1 step: 1696, loss is 1.4219215\n",
      "epoch: 1 step: 1697, loss is 1.6841222\n",
      "epoch: 1 step: 1698, loss is 1.1736845\n",
      "epoch: 1 step: 1699, loss is 1.0335425\n",
      "epoch: 1 step: 1700, loss is 1.0467652\n",
      "epoch: 1 step: 1701, loss is 1.3899386\n",
      "epoch: 1 step: 1702, loss is 1.1356802\n",
      "epoch: 1 step: 1703, loss is 1.1873693\n",
      "epoch: 1 step: 1704, loss is 1.2521685\n",
      "epoch: 1 step: 1705, loss is 1.4308505\n",
      "epoch: 1 step: 1706, loss is 1.1519145\n",
      "epoch: 1 step: 1707, loss is 1.154957\n",
      "epoch: 1 step: 1708, loss is 1.1044492\n",
      "epoch: 1 step: 1709, loss is 1.5408734\n",
      "epoch: 1 step: 1710, loss is 1.1230727\n",
      "epoch: 1 step: 1711, loss is 1.3745013\n",
      "epoch: 1 step: 1712, loss is 0.98707455\n",
      "epoch: 1 step: 1713, loss is 1.501725\n",
      "epoch: 1 step: 1714, loss is 1.1415843\n",
      "epoch: 1 step: 1715, loss is 1.336497\n",
      "epoch: 1 step: 1716, loss is 1.3905374\n",
      "epoch: 1 step: 1717, loss is 1.0677187\n",
      "epoch: 1 step: 1718, loss is 1.1887058\n",
      "epoch: 1 step: 1719, loss is 1.3241229\n",
      "epoch: 1 step: 1720, loss is 1.1440673\n",
      "epoch: 1 step: 1721, loss is 1.4345002\n",
      "epoch: 1 step: 1722, loss is 1.4070307\n",
      "epoch: 1 step: 1723, loss is 1.4056455\n",
      "epoch: 1 step: 1724, loss is 1.006643\n",
      "epoch: 1 step: 1725, loss is 1.2020202\n",
      "epoch: 1 step: 1726, loss is 1.2087944\n",
      "epoch: 1 step: 1727, loss is 1.1779933\n",
      "epoch: 1 step: 1728, loss is 0.8483858\n",
      "epoch: 1 step: 1729, loss is 1.1560891\n",
      "epoch: 1 step: 1730, loss is 1.0815971\n",
      "epoch: 1 step: 1731, loss is 1.369507\n",
      "epoch: 1 step: 1732, loss is 1.809761\n",
      "epoch: 1 step: 1733, loss is 1.2074594\n",
      "epoch: 1 step: 1734, loss is 1.1368284\n",
      "epoch: 1 step: 1735, loss is 1.3786504\n",
      "epoch: 1 step: 1736, loss is 1.6139177\n",
      "epoch: 1 step: 1737, loss is 1.5134553\n",
      "epoch: 1 step: 1738, loss is 1.2531371\n",
      "epoch: 1 step: 1739, loss is 1.3289577\n",
      "epoch: 1 step: 1740, loss is 1.3988173\n",
      "epoch: 1 step: 1741, loss is 1.4364911\n",
      "epoch: 1 step: 1742, loss is 1.1912773\n",
      "epoch: 1 step: 1743, loss is 1.4304256\n",
      "epoch: 1 step: 1744, loss is 1.1584365\n",
      "epoch: 1 step: 1745, loss is 1.3071651\n",
      "epoch: 1 step: 1746, loss is 1.3547219\n",
      "epoch: 1 step: 1747, loss is 1.1966257\n",
      "epoch: 1 step: 1748, loss is 1.0488737\n",
      "epoch: 1 step: 1749, loss is 1.5128356\n",
      "epoch: 1 step: 1750, loss is 1.3737752\n",
      "epoch: 1 step: 1751, loss is 1.281995\n",
      "epoch: 1 step: 1752, loss is 1.1361307\n",
      "epoch: 1 step: 1753, loss is 1.1660239\n",
      "epoch: 1 step: 1754, loss is 1.1787004\n",
      "epoch: 1 step: 1755, loss is 1.4019907\n",
      "epoch: 1 step: 1756, loss is 1.4035846\n",
      "epoch: 1 step: 1757, loss is 1.075823\n",
      "epoch: 1 step: 1758, loss is 1.1928262\n",
      "epoch: 1 step: 1759, loss is 1.1512692\n",
      "epoch: 1 step: 1760, loss is 1.1337645\n",
      "epoch: 1 step: 1761, loss is 1.3082854\n",
      "epoch: 1 step: 1762, loss is 1.1269244\n",
      "epoch: 1 step: 1763, loss is 1.4967635\n",
      "epoch: 1 step: 1764, loss is 1.2589101\n",
      "epoch: 1 step: 1765, loss is 1.325512\n",
      "epoch: 1 step: 1766, loss is 1.3126476\n",
      "epoch: 1 step: 1767, loss is 1.1790204\n",
      "epoch: 1 step: 1768, loss is 1.3660007\n",
      "epoch: 1 step: 1769, loss is 1.1876056\n",
      "epoch: 1 step: 1770, loss is 1.1052631\n",
      "epoch: 1 step: 1771, loss is 1.3946593\n",
      "epoch: 1 step: 1772, loss is 1.5869179\n",
      "epoch: 1 step: 1773, loss is 1.4644215\n",
      "epoch: 1 step: 1774, loss is 1.2015035\n",
      "epoch: 1 step: 1775, loss is 1.3416059\n",
      "epoch: 1 step: 1776, loss is 1.5456827\n",
      "epoch: 1 step: 1777, loss is 1.3131957\n",
      "epoch: 1 step: 1778, loss is 1.2165766\n",
      "epoch: 1 step: 1779, loss is 1.3731363\n",
      "epoch: 1 step: 1780, loss is 1.5608847\n",
      "epoch: 1 step: 1781, loss is 1.0284593\n",
      "epoch: 1 step: 1782, loss is 1.0105762\n",
      "epoch: 1 step: 1783, loss is 1.3656936\n",
      "epoch: 1 step: 1784, loss is 1.3851595\n",
      "epoch: 1 step: 1785, loss is 1.4062626\n",
      "epoch: 1 step: 1786, loss is 1.094767\n",
      "epoch: 1 step: 1787, loss is 1.4622529\n",
      "epoch: 1 step: 1788, loss is 1.0665267\n",
      "epoch: 1 step: 1789, loss is 1.0865732\n",
      "epoch: 1 step: 1790, loss is 1.2054977\n",
      "epoch: 1 step: 1791, loss is 1.3804083\n",
      "epoch: 1 step: 1792, loss is 1.1656184\n",
      "epoch: 1 step: 1793, loss is 1.1052822\n",
      "epoch: 1 step: 1794, loss is 0.8456131\n",
      "epoch: 1 step: 1795, loss is 1.2993178\n",
      "epoch: 1 step: 1796, loss is 1.1204298\n",
      "epoch: 1 step: 1797, loss is 1.2043227\n",
      "epoch: 1 step: 1798, loss is 1.4487773\n",
      "epoch: 1 step: 1799, loss is 1.031425\n",
      "epoch: 1 step: 1800, loss is 1.4576732\n",
      "epoch: 1 step: 1801, loss is 1.429253\n",
      "epoch: 1 step: 1802, loss is 1.971226\n",
      "epoch: 1 step: 1803, loss is 1.0908599\n",
      "epoch: 1 step: 1804, loss is 0.9847579\n",
      "epoch: 1 step: 1805, loss is 1.167846\n",
      "epoch: 1 step: 1806, loss is 1.0684575\n",
      "epoch: 1 step: 1807, loss is 1.1654888\n",
      "epoch: 1 step: 1808, loss is 1.2753575\n",
      "epoch: 1 step: 1809, loss is 0.90532327\n",
      "epoch: 1 step: 1810, loss is 1.3760648\n",
      "epoch: 1 step: 1811, loss is 1.2489953\n",
      "epoch: 1 step: 1812, loss is 1.2298932\n",
      "epoch: 1 step: 1813, loss is 1.102014\n",
      "epoch: 1 step: 1814, loss is 1.2636193\n",
      "epoch: 1 step: 1815, loss is 0.91748446\n",
      "epoch: 1 step: 1816, loss is 1.5203238\n",
      "epoch: 1 step: 1817, loss is 1.5358752\n",
      "epoch: 1 step: 1818, loss is 1.3291248\n",
      "epoch: 1 step: 1819, loss is 0.9183362\n",
      "epoch: 1 step: 1820, loss is 1.3926951\n",
      "epoch: 1 step: 1821, loss is 1.2906154\n",
      "epoch: 1 step: 1822, loss is 1.2106296\n",
      "epoch: 1 step: 1823, loss is 1.3377377\n",
      "epoch: 1 step: 1824, loss is 1.487588\n",
      "epoch: 1 step: 1825, loss is 1.4628423\n",
      "epoch: 1 step: 1826, loss is 1.2759589\n",
      "epoch: 1 step: 1827, loss is 1.4182216\n",
      "epoch: 1 step: 1828, loss is 1.4389536\n",
      "epoch: 1 step: 1829, loss is 1.2723372\n",
      "epoch: 1 step: 1830, loss is 0.90173835\n",
      "epoch: 1 step: 1831, loss is 1.1337832\n",
      "epoch: 1 step: 1832, loss is 1.0709238\n",
      "epoch: 1 step: 1833, loss is 1.2643529\n",
      "epoch: 1 step: 1834, loss is 1.1735591\n",
      "epoch: 1 step: 1835, loss is 1.5198653\n",
      "epoch: 1 step: 1836, loss is 1.1653099\n",
      "epoch: 1 step: 1837, loss is 1.0152732\n",
      "epoch: 1 step: 1838, loss is 0.91608423\n",
      "epoch: 1 step: 1839, loss is 1.2577986\n",
      "epoch: 1 step: 1840, loss is 1.600468\n",
      "epoch: 1 step: 1841, loss is 1.407977\n",
      "epoch: 1 step: 1842, loss is 1.3927346\n",
      "epoch: 1 step: 1843, loss is 1.1817359\n",
      "epoch: 1 step: 1844, loss is 1.5915002\n",
      "epoch: 1 step: 1845, loss is 1.27086\n",
      "epoch: 1 step: 1846, loss is 1.659921\n",
      "epoch: 1 step: 1847, loss is 1.0484917\n",
      "epoch: 1 step: 1848, loss is 1.4566606\n",
      "epoch: 1 step: 1849, loss is 1.2695823\n",
      "epoch: 1 step: 1850, loss is 1.2218467\n",
      "epoch: 1 step: 1851, loss is 1.58665\n",
      "epoch: 1 step: 1852, loss is 1.1953261\n",
      "epoch: 1 step: 1853, loss is 1.3165457\n",
      "epoch: 1 step: 1854, loss is 1.1624668\n",
      "epoch: 1 step: 1855, loss is 1.1142312\n",
      "epoch: 1 step: 1856, loss is 1.0882531\n",
      "epoch: 1 step: 1857, loss is 1.2935036\n",
      "epoch: 1 step: 1858, loss is 1.3415455\n",
      "epoch: 1 step: 1859, loss is 1.007157\n",
      "epoch: 1 step: 1860, loss is 1.1888285\n",
      "epoch: 1 step: 1861, loss is 1.1346369\n",
      "epoch: 1 step: 1862, loss is 1.4343108\n",
      "epoch: 1 step: 1863, loss is 1.0940998\n",
      "epoch: 1 step: 1864, loss is 1.0932161\n",
      "epoch: 1 step: 1865, loss is 1.1325103\n",
      "epoch: 1 step: 1866, loss is 1.259925\n",
      "epoch: 1 step: 1867, loss is 1.2006341\n",
      "epoch: 1 step: 1868, loss is 0.86012685\n",
      "epoch: 1 step: 1869, loss is 1.399402\n",
      "epoch: 1 step: 1870, loss is 1.0554316\n",
      "epoch: 1 step: 1871, loss is 1.5522469\n",
      "epoch: 1 step: 1872, loss is 1.186247\n",
      "epoch: 1 step: 1873, loss is 1.3959277\n",
      "epoch: 1 step: 1874, loss is 0.8069099\n",
      "epoch: 1 step: 1875, loss is 1.2175132\n"
     ]
    }
   ],
   "source": [
    "network = AlexNet(cfg.num_classes)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(is_grad=False, sparse=True, reduction=\"mean\")\n",
    "opt = nn.Momentum(network.trainable_params(), cfg.learning_rate, cfg.momentum)\n",
    "model = Model(network, loss, opt, metrics={\"Accuracy\": Accuracy()})  # test\n",
    "\n",
    "print(\"============== Starting Training ==============\")\n",
    "ds_train = create_dataset(data_path, cfg.batch_size, cfg.epoch_size, \"train\")\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps,\n",
    "                                 keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    " \n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_alexnet\", config=config_ck)\n",
    "model.train(cfg.epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor()],\n",
    "                dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\t\t\tcheckpoint_alexnet-graph.meta  home   opt   srv\r\n",
      "bin\t\t\t\tcifar-10-batches-bin\t       lib    proc  sys\r\n",
      "boot\t\t\t\tcifar-10-binary.tar.gz\t       lib64  root  tmp\r\n",
      "checkpoint_alexnet-1_1562.ckpt\tdev\t\t\t       media  run   usr\r\n",
      "checkpoint_alexnet-1_1875.ckpt\tetc\t\t\t       mnt    sbin  var\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Accuracy:{'Accuracy': 0.5426666666666666} ==============\n"
     ]
    }
   ],
   "source": [
    "# Check the name of the checkpoint file\n",
    "\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "\n",
    "param_dict = load_checkpoint(\"checkpoint_alexnet-1_1875.ckpt\")\n",
    "load_param_into_net(network, param_dict)\n",
    "\n",
    "ds_eval = create_dataset(data_path, cfg.batch_size, 1,\"test\")\n",
    "acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "print(\"============== Accuracy:{} ==============\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment related Accuracy: only 1 epoch is executed (required 90 minutes in my computer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
